{"file_contents":{"README.md":{"content":"# PDF Text Extractor and Tabulator\n\nA web application that extracts text from PDF files, processes it using OpenAI's GPT-4o model to identify structured information, and displays the results in a tabulated format with export options.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Architecture](#architecture)\n- [File Structure](#file-structure)\n- [How It Works](#how-it-works)\n- [Setup and Installation](#setup-and-installation)\n- [Technologies Used](#technologies-used)\n- [API Integration](#api-integration)\n- [Export Options](#export-options)\n- [Customization](#customization)\n\n## Overview\n\nThis application provides a streamlined way to extract structured information from PDF documents. It uses advanced AI to analyze text content and organize it into a tabular format that can be easily viewed and exported. The system automatically identifies entities and their categories, making it particularly useful for processing documents with consistent information patterns.\n\n## Features\n\n- **PDF Text Extraction**: Upload and extract text from PDF documents\n- **AI-Powered Analysis**: Process text with OpenAI's GPT-4o model to identify structured information\n- **Interactive UI**: Clean, modern interface with drag-and-drop file upload\n- **Real-time Processing**: See extracted text immediately after upload\n- **Tabulated Results**: View structured data in a clean, organized table\n- **Multiple Export Options**: Download results as JSON, CSV, or PDF\n- **Responsive Design**: Works on desktop and mobile devices\n\n## Architecture\n\nThe application follows a client-server architecture with:\n\n1. **Frontend**: HTML, CSS, and JavaScript for user interface\n2. **Backend**: Flask Python server to handle requests\n3. **Processing Layer**: PDF processing and AI analysis components\n4. **Export Layer**: Conversion utilities for different output formats\n\n## File Structure\n\n```\n├── app.py                  # Main Flask application\n├── llm_processor.py        # AI text processing with OpenAI\n├── pdf_processor.py        # PDF text extraction with PyPDF2\n├── export_utils.py         # Export functionality for various formats\n├── templates/\n│   └── index.html          # Main HTML template\n└── static/\n    ├── style.css           # CSS styling\n    └── script.js           # Frontend JavaScript\n```\n\n## How It Works\n\n1. **PDF Upload**:\n   - User uploads a PDF through the web interface\n   - The file is sent to the server for processing\n\n2. **Text Extraction**:\n   - The `pdf_processor.py` module extracts raw text from the PDF \n   - Uses PyPDF2 to access and read PDF content\n   - Extracted text is displayed to the user\n\n3. **AI Analysis**:\n   - User initiates processing with the \"Process with AI\" button\n   - The `llm_processor.py` module sends the text to OpenAI's GPT-4o model\n   - Custom prompting guides the AI to identify structured information\n   - The AI returns data categorized into predefined columns\n\n4. **Result Display**:\n   - Processed data is displayed in a table format\n   - Information is organized by category and extracted text\n\n5. **Data Export**:\n   - User selects preferred export format (JSON, CSV, PDF)\n   - The `export_utils.py` module handles conversion to the selected format\n   - Generated file is downloaded to the user's device\n\n## Setup and Installation\n\n1. **Prerequisites**:\n   - Python 3.11 or higher\n   - OpenAI API key\n\n2. **Installation**:\n   ```bash\n   # Clone the repository\n   git clone [repository-url]\n   cd pdf-text-extractor\n\n   # Install dependencies\n   pip install flask openai pandas pypdf2 reportlab\n   ```\n\n3. **Environment Variables**:\n   - Set your OpenAI API key:\n   ```\n   export OPENAI_API_KEY=\"your-api-key\"\n   ```\n\n4. **Running the Application**:\n   ```bash\n   python app.py\n   ```\n   - Access the application at http://localhost:5000\n\n## Technologies Used\n\n- **Flask**: Lightweight web framework for the backend\n- **OpenAI API**: GPT-4o model for text analysis\n- **PyPDF2**: PDF parsing and text extraction\n- **Pandas**: Data manipulation for structured information\n- **ReportLab**: PDF generation for exports\n- **Bootstrap**: Frontend styling framework\n- **JavaScript**: Frontend interactivity and API calls\n\n## API Integration\n\nThe application integrates with OpenAI's API to process the extracted text. The `llm_processor.py` file handles this integration with the following approach:\n\n1. Creates an OpenAI client instance with the API key\n2. Constructs a specialized prompt for document analysis\n3. Sends the prompt along with the extracted text to the GPT-4o model\n4. Processes the JSON response to extract structured data\n5. Returns the data in a format ready for display and export\n\n## Export Options\n\nThe application provides three export formats:\n\n1. **JSON**: Raw structured data in a machine-readable format\n2. **CSV**: Tabular data suitable for spreadsheet applications\n3. **PDF**: Formatted document with the extracted information in a table\n\nThe export functionality is handled in `export_utils.py` and the frontend JavaScript.\n\n## Customization\n\nThe AI prompt in `llm_processor.py` can be customized to extract different types of information or to format the output differently. The current prompt instructs the model to extract:\n\n- Category: Entity type (Person, Organization, Location, etc.)\n- Extracted Text: The identified data points or entities\n\nYou can customize this prompt to focus on specific types of information relevant to your documents.","size_bytes":5500},"app.py":{"content":"from flask import Flask, render_template, request, jsonify, send_file, Response\nimport os\nimport tempfile\nimport base64\nimport json\nfrom io import BytesIO\n\nfrom textract_processor import extract_text_from_pdf, extract_text_from_pdf_bytes, extract_structured_data_from_pdf_bytes\nfrom llm_processor import process_text_with_llm\nfrom structured_llm_processor import process_structured_data_with_llm\nfrom export_utils import export_to_pdf\n\napp = Flask(__name__)\n\n# Ensure templates directory exists\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.route('/extract', methods=['POST'])\ndef extract():\n    if 'pdf' not in request.files:\n        return jsonify({'error': 'No file uploaded'}), 400\n    \n    file = request.files['pdf']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n    \n    try:\n        # Extract structured data from PDF using Amazon Textract\n        pdf_bytes = file.read()\n        structured_data = extract_structured_data_from_pdf_bytes(pdf_bytes)\n        \n        # Return the new JSON format\n        return jsonify(structured_data)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\ndef summarize_commentary(text):\n    \"\"\"Summarize long commentary using GPT-4o\"\"\"\n    try:\n        import openai\n        import os\n        \n        client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        \n        prompt = f\"\"\"Summarize this financial document commentary in 2-3 complete sentences, preserving all key information:\n\n{text}\n\nInstructions:\n- Preserve ALL financial figures, percentages, dates, and company names\n- Keep the complete meaning and context\n- Use complete sentences that don't cut off mid-thought\n- Maintain the professional tone and key details\"\"\"\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=150,\n            temperature=0.2\n        )\n        \n        # Calculate and log cost for summarization\n        if hasattr(response, 'usage') and response.usage:\n            input_tokens = response.usage.prompt_tokens\n            output_tokens = response.usage.completion_tokens\n            input_cost = (input_tokens / 1_000_000) * 5.00  # GPT-4o input cost\n            output_cost = (output_tokens / 1_000_000) * 15.00  # GPT-4o output cost\n            total_cost = input_cost + output_cost\n            print(f\"Commentary summarization cost: ${total_cost:.6f} ({input_tokens} input + {output_tokens} output tokens)\")\n        \n        return response.choices[0].message.content.strip() if response.choices[0].message.content else \"\"\n    except Exception as e:\n        print(f\"Error summarizing commentary: {e}\")\n        return text[:200] + '...' if len(text) > 200 else text\n\ndef clean_csv_value(value):\n    \"\"\"Clean value for CSV format - escape commas and quotes\"\"\"\n    if not value:\n        return \"\"\n    \n    # Convert to string and clean up\n    value_str = str(value).strip()\n    \n    # Escape quotes by doubling them\n    value_str = value_str.replace('\"', '\"\"')\n    \n    # If value contains comma, newline, or quote, wrap in quotes\n    if ',' in value_str or '\\n' in value_str or '\"' in value_str:\n        value_str = f'\"{value_str}\"'\n    \n    return value_str\n\ndef find_relevant_document_context(field, value, document_text):\n    \"\"\"Find relevant context from document text for a specific field/value pair\"\"\"\n    if not document_text or not field:\n        return \"\"\n    \n    field_lower = field.lower().replace('_', ' ')\n    value_lower = str(value).lower().replace('$', '').replace('%', '').replace(',', '')\n    \n    # Extract numeric part if value contains numbers\n    import re\n    numeric_parts = re.findall(r'\\d+\\.?\\d*', value_lower)\n    \n    best_matches = []\n    \n    # Look for text segments that mention the field or value\n    for i, line in enumerate(document_text):\n        line_lower = line.lower()\n        score = 0\n        \n        # High priority: exact value match\n        if value_lower and len(value_lower) > 2 and value_lower in line_lower:\n            score += 10\n        \n        # Medium priority: numeric match\n        for num in numeric_parts:\n            if len(num) > 1 and num in line_lower:\n                score += 5\n        \n        # Lower priority: field word matches\n        field_words = [word for word in field_lower.split() if len(word) > 2]\n        for word in field_words:\n            if word in line_lower:\n                score += 2\n        \n        # If we found a relevant match, store it with context\n        if score >= 7:  # Only include high-confidence matches\n            # Get context around the matching line\n            start_idx = max(0, i - 1)\n            end_idx = min(len(document_text), i + 2)\n            context_lines = document_text[start_idx:end_idx]\n            \n            # Join and clean up the context\n            context = ' '.join(context_lines).strip()\n            \n            best_matches.append({\n                'text': context,\n                'score': score,\n                'line_index': i\n            })\n    \n    # Sort by score and return the best match, truncated if too long\n    if best_matches:\n        best_matches.sort(key=lambda x: x['score'], reverse=True)\n        best_context = best_matches[0]['text']\n        \n        # Truncate if too long but keep complete sentences\n        if len(best_context) > 300:\n            sentences = best_context.replace('!', '.').replace('?', '.').split('.')\n            complete_text = \"\"\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if sentence and len(complete_text + sentence) < 250:\n                    complete_text += sentence + \". \"\n                else:\n                    break\n            \n            return complete_text.strip() if complete_text else best_context[:300] + '...'\n        else:\n            return best_context\n    \n    return \"\"\n\ndef find_relevant_document_text(row_data, document_text):\n    \"\"\"Find relevant text from document that mentions this data point\"\"\"\n    field = row_data.get('field', '').lower()\n    value = str(row_data.get('value', '')).lower()\n    \n    # Clean field and value for better matching\n    field_words = [word for word in field.replace('_', ' ').split() if len(word) > 2]\n    value_clean = value.replace('$', '').replace('%', '').replace(',', '').strip()\n    \n    # Extract numeric part if value contains numbers\n    import re\n    numeric_part = re.findall(r'\\d+\\.?\\d*', value_clean)\n    \n    best_matches = []\n    \n    # Look for text segments that mention the value or field\n    for i, line in enumerate(document_text):\n        line_lower = line.lower()\n        line_clean = _clean_superscript_numbers(line_lower)\n        score = 0\n        \n        # High priority: exact value match\n        if value_clean and len(value_clean) > 2 and value_clean in line_clean:\n            score += 10\n        \n        # Medium priority: numeric match\n        for num in numeric_part:\n            if len(num) > 1 and num in line_clean:\n                score += 7\n        \n        # Lower priority: field word matches\n        for word in field_words:\n            if word in line_lower:\n                score += 2\n        \n        # If we found a relevant match, store it with context\n        if score >= 9:  # Only include very high-confidence matches\n            # Get targeted context around the matching line\n            start_idx = max(0, i - 1)\n            end_idx = min(len(document_text), i + 3)\n            context_lines = document_text[start_idx:end_idx]\n            \n            # Join and clean up the context\n            context = ' '.join(context_lines).strip()\n            context = _clean_superscript_numbers(context)\n            \n            best_matches.append({\n                'text': context,\n                'score': score,\n                'line_index': i\n            })\n    \n    # Sort by score and return the best match\n    if best_matches:\n        best_matches.sort(key=lambda x: x['score'], reverse=True)\n        best_context = best_matches[0]['text']\n        \n        # Truncate if too long but keep complete sentences\n        if len(best_context) > 400:\n            sentences = best_context.replace('!', '.').replace('?', '.').split('.')\n            complete_text = \"\"\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if sentence and len(complete_text + sentence) < 350:\n                    complete_text += sentence + \". \"\n                else:\n                    break\n            \n            if complete_text:\n                return complete_text.strip()\n            else:\n                return best_context[:400] + '...'\n        else:\n            return best_context\n    \n    return ''  # No relevant matches found\n\ndef _clean_superscript_numbers(text):\n    \"\"\"Remove superscript numbers from text for better matching\"\"\"\n    import re\n    \n    # Remove superscript numbers (Unicode superscript characters)\n    superscript_pattern = r'[⁰¹²³⁴⁵⁶⁷⁸⁹]+'\n    text = re.sub(superscript_pattern, '', text)\n    \n    # Remove common footnote reference patterns\n    footnote_patterns = [\n        r'\\(\\d+\\)',    # (1), (2), etc.\n        r'\\[\\d+\\]',    # [1], [2], etc.\n        r'\\*+',        # *, **, ***, etc.\n    ]\n    \n    for pattern in footnote_patterns:\n        text = re.sub(pattern, '', text)\n    \n    return ' '.join(text.split())\n\ndef get_unmatched_document_text(df_data, document_text):\n    \"\"\"Get document text that doesn't match any extracted data\"\"\"\n    used_indices = set()\n    \n    # Mark lines that were used for commentary (with context)\n    for row in df_data:\n        if row.get('commentary'):\n            commentary_sample = row['commentary'][:100].lower()\n            for i, line in enumerate(document_text):\n                if commentary_sample in line.lower():\n                    # Mark this line and surrounding context as used\n                    for j in range(max(0, i-1), min(len(document_text), i+2)):\n                        used_indices.add(j)\n    \n    # Collect unused lines in meaningful paragraphs\n    unmatched_paragraphs = []\n    current_paragraph = []\n    \n    for i, line in enumerate(document_text):\n        if i not in used_indices and len(line.strip()) > 15:\n            current_paragraph.append(line.strip())\n        else:\n            # End of paragraph - save if substantial\n            if current_paragraph:\n                paragraph_text = ' '.join(current_paragraph)\n                if len(paragraph_text) > 50:  # Only keep substantial paragraphs\n                    unmatched_paragraphs.append(paragraph_text)\n                current_paragraph = []\n    \n    # Don't forget the last paragraph\n    if current_paragraph:\n        paragraph_text = ' '.join(current_paragraph)\n        if len(paragraph_text) > 50:\n            unmatched_paragraphs.append(paragraph_text)\n    \n    # Limit and truncate paragraphs for readability with complete sentences\n    final_chunks = []\n    for paragraph in unmatched_paragraphs[:3]:  # Limit to 3 substantial chunks\n        if len(paragraph) > 500:\n            # Find complete sentences to avoid cutting off mid-sentence\n            sentences = paragraph.replace('!', '.').replace('?', '.').split('.')\n            complete_paragraph = \"\"\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if sentence and len(complete_paragraph + sentence) < 450:\n                    complete_paragraph += sentence + \". \"\n                else:\n                    break\n            \n            if complete_paragraph and len(complete_paragraph) > 50:\n                final_chunks.append(complete_paragraph.strip())\n            else:\n                # Fallback: truncate at word boundary\n                truncated = paragraph[:450]\n                last_space = truncated.rfind(' ')\n                if last_space > 300:\n                    final_chunks.append(truncated[:last_space] + '...')\n        else:\n            final_chunks.append(paragraph)\n    \n    return final_chunks\n\n@app.route('/process_stream', methods=['POST'])\ndef process_stream():\n    \"\"\"Streaming endpoint for progressive data processing in XLSX format\"\"\"\n    data = request.json\n    if not data:\n        return jsonify({'error': 'No data provided'}), 400\n    \n    def generate():\n        try:\n            # Process the structured JSON data \n            result = process_structured_data_with_llm(data)\n            \n            # Initialize CSV-like output for XLSX format\n            csv_output = []\n            row_counter = 1\n            commentary_collection = []  # Collect all commentary for single row\n            \n            # Start with header row\n            header_row = f\"row{row_counter}: source,type,field,value,page,context\"\n            csv_output.append(header_row)\n            yield f\"data: {json.dumps({'type': 'header', 'content': header_row})}\\n\\n\"\n            row_counter += 1\n            \n            # Process tables with enhanced financial data extraction\n            if 'processed_tables' in result and result['processed_tables']:\n                for i, table in enumerate(result['processed_tables']):\n                    if table.get('structured_table') and not table['structured_table'].get('error'):\n                        table_data = table['structured_table']\n                        page = table.get('page', 'N/A')\n                        \n                        # Stream each table data row in XLSX format (no individual context)\n                        for key, value in table_data.items():\n                            if key != 'error' and value:\n                                # Create CSV-formatted row without individual context\n                                csv_row = f\"row{row_counter}: Table {i+1},Table Data,{key},{clean_csv_value(str(value))},{page},\"\n                                csv_output.append(csv_row)\n                                yield f\"data: {json.dumps({'type': 'row', 'content': csv_row})}\\n\\n\"\n                                row_counter += 1\n            \n            # Process key-value pairs\n            if 'processed_key_values' in result and result['processed_key_values']:\n                kv_data = result['processed_key_values'].get('structured_key_values', {})\n                if kv_data and not kv_data.get('error'):\n                    for key, value in kv_data.items():\n                        if key != 'error' and value:\n                            # Create CSV-formatted row without individual context\n                            csv_row = f\"row{row_counter}: Key-Value Pairs,Structured Data,{key},{clean_csv_value(str(value))},N/A,\"\n                            csv_output.append(csv_row)\n                            yield f\"data: {json.dumps({'type': 'row', 'content': csv_row})}\\n\\n\"\n                            row_counter += 1\n            \n            # Process document text facts and collect commentary\n            if 'processed_document_text' in result and result['processed_document_text']:\n                for chunk_idx, chunk in enumerate(result['processed_document_text']):\n                    if 'extracted_facts' in chunk and not chunk['extracted_facts'].get('error'):\n                        facts = chunk['extracted_facts']\n                        for key, value in facts.items():\n                            if key != 'error' and value:\n                                # Determine if this is footnote content\n                                data_type = 'Footnote' if 'footnote' in key.lower() else 'Financial Data'\n                                field_name = key.replace('_Footnote', ' (Footnote)').replace('Footnote_', 'Footnote: ')\n                                \n                                # Create CSV-formatted row without individual context\n                                csv_row = f\"row{row_counter}: Text Chunk {chunk_idx+1},{data_type},{field_name},{clean_csv_value(str(value))},N/A,\"\n                                csv_output.append(csv_row)\n                                yield f\"data: {json.dumps({'type': 'row', 'content': csv_row})}\\n\\n\"\n                                row_counter += 1\n                                \n                                # Collect for general commentary\n                                commentary_collection.append(f\"{field_name}: {value}\")\n            \n            # Collect document text for general commentary\n            document_text_list = data.get('document_text', [])\n            if document_text_list:\n                # Clean and organize document text into paragraphs\n                text_content = []\n                for i, line in enumerate(document_text_list):\n                    line = line.strip()\n                    if line and len(line) > 20:  # Only include substantial lines\n                        text_content.append(line)\n                \n                # Group into meaningful paragraphs\n                if text_content:\n                    commentary_collection.append(\"\\n\\nDocument Overview:\")\n                    paragraph_break_counter = 0\n                    for line in text_content[:10]:  # Limit to first 10 substantial lines\n                        commentary_collection.append(line)\n                        paragraph_break_counter += 1\n                        if paragraph_break_counter % 3 == 0:  # Add paragraph break every 3 lines\n                            commentary_collection.append(\"\")\n            \n            # Process standalone footnotes and collect for commentary\n            if 'footnotes' in data and data['footnotes']:\n                footnote_texts = []\n                for footnote in data['footnotes']:\n                    footnote_content = footnote.get('content', '')\n                    marker = footnote.get('marker', 'N/A')\n                    footnote_texts.append(f\"Footnote {marker}: {footnote_content}\")\n                \n                if footnote_texts:\n                    commentary_collection.append(\"\\n\\nFootnotes:\")\n                    commentary_collection.extend(footnote_texts)\n            \n            # Create single general commentary row\n            if commentary_collection:\n                # Join all commentary with proper paragraph formatting\n                full_commentary = \"\\n\\n\".join(commentary_collection)\n                \n                # Clean and format the commentary for Excel cell\n                formatted_commentary = clean_csv_value(full_commentary)\n                \n                # Create single general commentary CSV row\n                csv_row = f\"row{row_counter}: General Commentary,Document Summary,Overall Document Commentary,{formatted_commentary},N/A,Complete document analysis and extracted information summary\"\n                csv_output.append(csv_row)\n                yield f\"data: {json.dumps({'type': 'row', 'content': csv_row})}\\n\\n\"\n                row_counter += 1\n            \n            # Add cost summary if available\n            cost_summary = result.get('cost_summary', {})\n            if cost_summary:\n                total_cost = cost_summary.get('total_cost_usd', 0)\n                total_tokens = cost_summary.get('total_tokens', 0)\n                api_calls = cost_summary.get('api_calls', 0)\n                \n                # Create CSV-formatted row for cost summary\n                csv_row = f\"row{row_counter}: Cost Summary,Processing Cost,Total LLM Cost,{clean_csv_value(f'${total_cost:.6f}')},N/A,{clean_csv_value(f'Tokens: {total_tokens:,} | API Calls: {api_calls}')}\"\n                csv_output.append(csv_row)\n                yield f\"data: {json.dumps({'type': 'row', 'content': csv_row})}\\n\\n\"\n                row_counter += 1\n            \n            # Send completion signal (no large CSV data to avoid JSON truncation)\n            yield f\"data: {json.dumps({'type': 'complete', 'total_rows': row_counter - 1, 'cost_summary': cost_summary})}\\n\\n\"\n            \n        except Exception as e:\n            yield f\"data: {json.dumps({'status': 'error', 'error': str(e)})}\\n\\n\"\n    \n    return Response(generate(), mimetype='text/event-stream', headers={\n        'Cache-Control': 'no-cache',\n        'Connection': 'keep-alive',\n        'Access-Control-Allow-Origin': '*',\n        'Access-Control-Allow-Headers': 'Cache-Control',\n        'X-Accel-Buffering': 'no'\n    })\n\n@app.route('/export_xlsx', methods=['POST'])\ndef export_xlsx():\n    \"\"\"Export CSV data to XLSX format with specific sheet name\"\"\"\n    try:\n        import io\n        import csv\n        from io import StringIO\n        from openpyxl import Workbook\n        from openpyxl.styles import Font, PatternFill, Alignment\n        \n        data = request.json\n        if not data:\n            return jsonify({'error': 'No data provided'}), 400\n            \n        csv_data = data.get('csv_data', '')\n        \n        if not csv_data:\n            return jsonify({'error': 'No CSV data provided'}), 400\n        \n        # Parse CSV data\n        lines = csv_data.strip().split('\\n')\n        if not lines:\n            return jsonify({'error': 'Empty CSV data'}), 400\n        \n        # Create workbook with specific sheet name\n        wb = Workbook()\n        ws = wb.active\n        if ws is not None:\n            ws.title = \"Extracted_data_comments\"\n        \n        # Parse and add data to worksheet\n        if ws is not None:\n            for row_idx, line in enumerate(lines, 1):\n                if line.startswith('row'):\n                    # Extract the actual CSV content after \"rowN: \"\n                    colon_idx = line.find(': ')\n                    if colon_idx != -1:\n                        csv_content = line[colon_idx + 2:]\n                        \n                        # Parse CSV row (handle quoted values with commas)\n                        try:\n                            csv_reader = csv.reader(StringIO(csv_content))\n                            row_data = next(csv_reader)\n                            \n                            # Add to worksheet\n                            for col_idx, cell_value in enumerate(row_data, 1):\n                                cell = ws.cell(row=row_idx, column=col_idx, value=cell_value)\n                                \n                                # Style header row\n                                if row_idx == 1:\n                                    cell.font = Font(bold=True)\n                                    cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n                                    cell.alignment = Alignment(horizontal=\"center\")\n                        except Exception as parse_error:\n                            print(f\"Error parsing CSV row: {parse_error}\")\n                            continue\n        \n            # Auto-adjust column widths\n            try:\n                for column in ws.columns:\n                    max_length = 0\n                    column_letter = column[0].column_letter\n                    for cell in column:\n                        try:\n                            if cell.value and len(str(cell.value)) > max_length:\n                                max_length = len(str(cell.value))\n                        except:\n                            pass\n                    adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters\n                    ws.column_dimensions[column_letter].width = adjusted_width\n            except Exception as width_error:\n                print(f\"Error adjusting column widths: {width_error}\")\n        \n        # Save to memory\n        output = io.BytesIO()\n        wb.save(output)\n        output.seek(0)\n        \n        # Return file\n        return send_file(\n            output,\n            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n            as_attachment=True,\n            download_name='extracted_data_comments.xlsx'\n        )\n        \n    except Exception as e:\n        return jsonify({'error': f'Failed to generate XLSX: {str(e)}'}), 500\n\n@app.route('/process', methods=['POST'])\ndef process():\n    data = request.json\n    if not data:\n        return jsonify({'error': 'No data provided'}), 400\n    \n    try:\n        import pandas as pd\n        \n        # Process the structured JSON data with separate LLM calls and commentary matching\n        result = process_structured_data_with_llm(data)\n        \n        # Use the enhanced data with commentary if available\n        if 'enhanced_data_with_commentary' in result and result['enhanced_data_with_commentary']:\n            clean_data = result['enhanced_data_with_commentary']\n            \n            # Add general commentary as a separate row if it exists\n            if result.get('general_commentary'):\n                clean_data.append({\n                    'source': 'Document Text',\n                    'type': 'General Commentary',\n                    'field': 'Unmatched Commentary',\n                    'value': result['general_commentary'][:500] + '...' if len(result['general_commentary']) > 500 else result['general_commentary'],\n                    'page': 'N/A',\n                    'commentary': '',\n                    'has_commentary': False\n                })\n        else:\n            # Fallback to original processing if enhanced data is not available\n            df_data = []\n            \n            # Process tables\n            if 'processed_tables' in result and result['processed_tables']:\n                for i, table in enumerate(result['processed_tables']):\n                    if table.get('structured_table') and not table['structured_table'].get('error'):\n                        table_data = table['structured_table']\n                        page = table.get('page', 'N/A')\n                        \n                        # Handle different table structures\n                        if isinstance(table_data, dict):\n                            for key, value in table_data.items():\n                                if key != 'error':\n                                    df_data.append({\n                                        'source': f'Table {i+1}',\n                                        'type': 'Table Data',\n                                        'field': key,\n                                        'value': str(value) if value else '',\n                                        'page': page,\n                                        'commentary': '',\n                                        'has_commentary': False\n                                    })\n            \n            # Process key-value pairs\n            if 'processed_key_values' in result and result['processed_key_values']:\n                kv_data = result['processed_key_values'].get('structured_key_values', {})\n                if kv_data and not kv_data.get('error'):\n                    for key, value in kv_data.items():\n                        if key != 'error':\n                            df_data.append({\n                                'source': 'Key-Value Pairs',\n                                'type': 'Structured Data',\n                                'field': key,\n                                'value': str(value) if value else '',\n                                'page': 'N/A',\n                                'commentary': '',\n                                'has_commentary': False\n                            })\n            \n            # Process document text with tabulation\n            if 'processed_document_text' in result and result['processed_document_text']:\n                for chunk_idx, chunk in enumerate(result['processed_document_text']):\n                    # Handle tabulated document text structure\n                    if 'table_headers' in chunk and 'table_rows' in chunk:\n                        headers = chunk['table_headers']\n                        rows = chunk['table_rows']\n                        \n                        # Add document text table structure\n                        df_data.append({\n                            'source': f'Document Text {chunk_idx+1}',\n                            'type': 'Document Table',\n                            'field': 'Headers',\n                            'value': ' | '.join(headers),\n                            'page': 'N/A',\n                            'commentary': 'Tabulated document content',\n                            'has_commentary': True,\n                            'is_table_header': True,\n                            'table_id': f'doc_{chunk_idx}',\n                            'headers': headers,\n                            'rows': rows\n                        })\n                        \n                        # Add individual data points from document table\n                        for row_idx, row in enumerate(rows):\n                            for col_idx, cell_value in enumerate(row):\n                                if col_idx < len(headers) and cell_value:\n                                    df_data.append({\n                                        'source': f'Document Text {chunk_idx+1}',\n                                        'type': 'Document Data',\n                                        'field': f'{headers[col_idx]}_Row_{row_idx+1}',\n                                        'value': str(cell_value),\n                                        'page': 'N/A',\n                                        'commentary': '',\n                                        'has_commentary': False,\n                                        'table_id': f'doc_{chunk_idx}'\n                                    })\n                    \n                    # Also handle extracted facts if available\n                    if 'extracted_facts' in chunk and not chunk['extracted_facts'].get('error'):\n                        facts = chunk['extracted_facts']\n                        for key, value in facts.items():\n                            if key != 'error' and value:\n                                df_data.append({\n                                    'source': f'Text Chunk {chunk_idx+1}',\n                                    'type': 'Financial Data',\n                                    'field': key,\n                                    'value': str(value),\n                                    'page': 'N/A',\n                                    'commentary': '',\n                                    'has_commentary': False\n                                })\n            \n            # Clean and prepare data\n            if df_data:\n                # Filter out empty values directly from the list\n                clean_data = [\n                    item for item in df_data \n                    if item.get('value', '').strip() and item.get('value') != 'nan'\n                ]\n            else:\n                clean_data = []\n        \n        # Return both original result and clean DataFrame data\n        response = {\n            **result,\n            'dataframe_data': clean_data,\n            'total_rows': len(clean_data)\n        }\n        \n        return jsonify(response)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/extract_structured', methods=['POST'])\ndef extract_structured():\n    \"\"\"Extract structured data from PDF using Amazon Textract\"\"\"\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file uploaded'}), 400\n    \n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n    \n    if not file.filename or not file.filename.lower().endswith('.pdf'):\n        return jsonify({'error': 'Only PDF files are supported'}), 400\n    \n    try:\n        # Extract structured data using Textract\n        pdf_bytes = file.read()\n        structured_data = extract_structured_data_from_pdf_bytes(pdf_bytes)\n        \n        return jsonify({\n            'success': True,\n            'structured_data': structured_data\n        })\n        \n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/export/pdf', methods=['POST'])\ndef export_pdf():\n    data = request.json\n    if not data or 'data' not in data:\n        return jsonify({'error': 'No data provided'}), 400\n    \n    try:\n        import pandas as pd\n        df = pd.DataFrame(data['data'])\n        pdf_bytes = export_to_pdf(df)\n        \n        return jsonify({\n            'pdf': base64.b64encode(pdf_bytes).decode('utf-8')\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)","size_bytes":32646},"export_utils.py":{"content":"import json\nimport base64\nimport pandas as pd\nfrom io import BytesIO\nfrom reportlab.lib import colors\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndef export_to_pdf(df):\n    \"\"\"\n    Export a DataFrame to PDF format.\n    \n    Args:\n        df (pandas.DataFrame): DataFrame to export\n        \n    Returns:\n        bytes: PDF content as bytes\n    \"\"\"\n    buffer = BytesIO()\n    \n    # Create the PDF document with landscape orientation for better table display\n    doc = SimpleDocTemplate(buffer, pagesize=letter, leftMargin=30, rightMargin=30, topMargin=30, bottomMargin=30)\n    elements = []\n    \n    # Get the style for paragraphs\n    styles = getSampleStyleSheet()\n    title_style = styles['Heading1']\n    \n    # Add a title\n    elements.append(Paragraph(\"Extracted Information\", title_style))\n    \n    # Handle the new table format where column names might vary\n    # First, get all unique column names across all rows\n    all_columns = set()\n    for _, row in df.iterrows():\n        all_columns.update(row.keys())\n    \n    # Ensure \"Category\" is the first column\n    column_order = [\"Category\"]\n    value_columns = sorted([col for col in all_columns if col != \"Category\" and col.startswith(\"Value\")])\n    column_order.extend(value_columns)\n    \n    # Create a new DataFrame with the ordered columns\n    ordered_df = pd.DataFrame(columns=column_order)\n    for idx, row in df.iterrows():\n        ordered_row = {}\n        for col in column_order:\n            ordered_row[col] = row.get(col, \"\") if col in row else \"\"\n        ordered_df.loc[idx] = ordered_row\n    \n    # Convert DataFrame to list for table\n    data = [ordered_df.columns.tolist()] + ordered_df.values.tolist()\n    \n    # Create the table with column widths\n    col_widths = [120]  # Width for Category column\n    col_widths.extend([180] * (len(column_order) - 1))  # Width for Value columns\n    table = Table(data, colWidths=col_widths)\n    \n    # Add style to the table\n    style = TableStyle([\n        # Header row\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n        \n        # Category column\n        ('BACKGROUND', (0, 1), (0, -1), colors.lightgrey),\n        ('FONTNAME', (0, 1), (0, -1), 'Helvetica-Bold'),\n        \n        # Data cells\n        ('BACKGROUND', (1, 1), (-1, -1), colors.beige),\n        \n        # Grid and borders\n        ('BOX', (0, 0), (-1, -1), 1, colors.black),\n        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n        \n        # Text alignment\n        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n        ('ALIGN', (0, 1), (0, -1), 'LEFT'),  # Left align category names\n        ('ALIGN', (1, 1), (-1, -1), 'LEFT'),  # Left align values\n        \n        # Padding\n        ('TOPPADDING', (0, 0), (-1, -1), 8),\n        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n        ('LEFTPADDING', (0, 0), (-1, -1), 6),\n        ('RIGHTPADDING', (0, 0), (-1, -1), 6),\n    ])\n    table.setStyle(style)\n    \n    # Add the table to the elements\n    elements.append(table)\n    \n    # Build the PDF\n    doc.build(elements)\n    \n    # Get the PDF content\n    pdf_content = buffer.getvalue()\n    buffer.close()\n    \n    return pdf_content\n\ndef create_download_link(content, filename, mime_type):\n    \"\"\"\n    Create an HTML download link for the content.\n    \n    Args:\n        content: The content to download\n        filename (str): The name of the file\n        mime_type (str): The MIME type of the file\n        \n    Returns:\n        str: HTML download link\n    \"\"\"\n    if mime_type == 'application/pdf':\n        # For PDF, content is already base64 encoded\n        href = f'data:{mime_type};base64,{content}'\n    else:\n        # For other types, encode the content\n        b64 = base64.b64encode(content.encode()).decode()\n        href = f'data:{mime_type};base64,{b64}'\n    \n    # Create the download link\n    download_link = f'<a href=\"{href}\" download=\"{filename}\">Download {filename}</a>'\n    \n    return download_link\n","size_bytes":4243},"llm_processor.py":{"content":"import os\nimport json\nfrom openai import OpenAI\n\n\ndef process_text_with_llm(text):\n    \"\"\"\n    Process the extracted text with an LLM to identify structured information.\n    \n    Args:\n        text (str): The text extracted from the PDF\n        \n    Returns:\n        list: A list of dictionaries containing structured data\n    \"\"\"\n    # Get API key from environment\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OpenAI API key not found in environment variables\")\n\n    try:\n        # Create OpenAI client\n        client = OpenAI(api_key=api_key)\n\n        # Build the comprehensive prompt for maximum data extraction\n        system_prompt = \"\"\"\n        You are an elite data extraction specialist. Your mission is to extract EVERY SINGLE piece of information from the provided document text and organize it into the most comprehensive table possible.\n\n        CRITICAL REQUIREMENTS:\n        1. Extract 100% of ALL information - leave nothing out\n        2. Create separate rows for EVERY distinct data point, fact, number, name, date, or detail\n        3. Break down complex information into granular components\n        4. Include ALL numbers, percentages, financial figures, dates, names, locations, descriptions\n        5. Process ALL sections, headers, footnotes, tables, lists, and annotations\n        6. Extract metadata like document types, sections, subsections, and structural elements\n        7. Capture ALL relationships, comparisons, and contextual information\n\n        COMPREHENSIVE EXTRACTION APPROACH:\n        - Company/Organization Information: Names, addresses, contact details, registration numbers, etc.\n        - Financial Data: All revenues, costs, profits, ratios, growth rates, projections, etc.\n        - Personnel: All names, titles, roles, departments, contact information\n        - Dates & Time: All dates, periods, quarters, years, deadlines, timelines\n        - Legal/Regulatory: Compliance items, regulations, legal entities, jurisdictions\n        - Operational: Business units, products, services, markets, segments\n        - Performance Metrics: All KPIs, statistics, measurements, benchmarks\n        - Strategic Information: Goals, initiatives, plans, forecasts, risks\n        - Technical Details: Specifications, processes, methodologies, systems\n        - Geographic: All locations, regions, markets, addresses, jurisdictions\n\n        TABLE STRUCTURE:\n        - \"Category\": Descriptive label for the type of information\n        - \"Value 1\", \"Value 2\", \"Value 3\", etc.: Use as many columns as needed\n        - Create separate rows for each distinct piece of information\n        - Be granular - break down complex items into individual components\n\n        EXAMPLES OF GRANULAR EXTRACTION:\n        - If document mentions \"Q4 2024 revenue of $115.5 million\", create separate rows for:\n          * Quarter Period: Q4 2024\n          * Revenue Amount: $115.5 million\n          * Revenue Period: Q4 2024\n          * Currency Type: USD\n        - For addresses, separate into: Street, City, State, Country, Postal Code\n        - For names, consider: Full Name, First Name, Last Name, Title\n\n        Your output must be a valid JSON object:\n        {\n          \"data\": [\n            {\"Category\": \"category_name\", \"Value 1\": \"value1\", \"Value 2\": \"value2\", ...},\n            {\"Category\": \"another_category\", \"Value 1\": \"value1\", ...}\n          ]\n        }\n\n        ABSOLUTE MANDATE: Extract EVERYTHING. Be exhaustive. Create as many rows as needed to capture ALL information.\n        \"\"\"\n\n        user_prompt = f\"\"\"\n        Here is the extracted text from a PDF document using LlamaParse:\n\n        {text}\n\n        EXTRACTION INSTRUCTIONS:\n        1. Create MANY ROWS - aim for 50+ rows minimum if the document has substantial content\n        2. Create MULTIPLE COLUMNS - use as many Value columns as needed (Value 1, Value 2, Value 3, Value 4, Value 5, etc.)\n        3. Break down EVERY piece of information into separate rows:\n           - Each financial figure gets its own row\n           - Each date gets its own row  \n           - Each name gets its own row\n           - Each address component gets its own row\n           - Each percentage or ratio gets its own row\n           - Each section header gets its own row\n           - Each business metric gets its own row\n\n        EXAMPLES OF GRANULAR BREAKDOWN:\n        - Company \"Life360, Inc.\" becomes multiple rows:\n          * Company Legal Name: Life360, Inc.\n          * Company Short Name: Life360\n          * Company Type: Inc.\n          * Industry Classification: Technology/Software\n        \n        - \"Q4 2024 Revenue $115.5 million\" becomes multiple rows:\n          * Reporting Period: Q4 2024\n          * Revenue Quarter: Q4\n          * Revenue Year: 2024\n          * Revenue Amount: $115.5 million\n          * Revenue Currency: USD\n          * Revenue Value (Numeric): 115.5\n          * Revenue Unit: Million\n\n        - Any table in the document should be broken down cell by cell\n        - Any list should have each item as a separate row\n        - Any multi-part information should be separated into components\n\n        CREATE A COMPREHENSIVE MULTI-DIMENSIONAL TABLE with maximum rows and columns.\n        \"\"\"\n\n        # Send the prompt to the model using gpt-4o-mini for optimal performance\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\n                \"role\": \"system\",\n                \"content\": system_prompt\n            }, {\n                \"role\": \"user\",\n                \"content\": user_prompt\n            }],\n            temperature=0.1,\n            max_tokens=4000,\n            response_format={\"type\": \"json_object\"})\n\n        # Calculate and log cost for comprehensive processing\n        if hasattr(response, 'usage') and response.usage:\n            input_tokens = response.usage.prompt_tokens\n            output_tokens = response.usage.completion_tokens\n            input_cost = (input_tokens /\n                          1_000_000) * 0.150  # GPT-4o-mini input cost\n            output_cost = (output_tokens /\n                           1_000_000) * 0.600  # GPT-4o-mini output cost\n            total_cost = input_cost + output_cost\n            print(\n                f\"Comprehensive LLM processing cost: ${total_cost:.6f} ({input_tokens} input + {output_tokens} output tokens)\"\n            )\n\n        # Extract JSON from the response\n        if response and response.choices and len(response.choices) > 0:\n            response_content = response.choices[0].message.content\n\n            # Parse the JSON\n            if response_content:\n                structured_data = json.loads(response_content)\n\n                # Ensure we have the data in the expected format\n                if \"data\" in structured_data and isinstance(\n                        structured_data[\"data\"], list):\n                    return structured_data[\"data\"]\n                elif isinstance(structured_data, list):\n                    return structured_data\n                else:\n                    # If we got a JSON object but not in the expected format, try to extract data\n                    for key, value in structured_data.items():\n                        if isinstance(value, list) and len(value) > 0:\n                            return value\n\n                    # If we couldn't find a suitable list, wrap the whole object in a list\n                    return [structured_data]\n            else:\n                raise ValueError(\"Empty response from OpenAI API\")\n        else:\n            raise ValueError(\"Invalid response from OpenAI API\")\n\n    except Exception as e:\n        raise Exception(f\"Error processing text with LLM: {str(e)}\")\n","size_bytes":7722},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"openai>=1.79.0\",\n    \"pandas>=2.2.3\",\n    \"pypdf2>=3.0.1\",\n    \"reportlab>=4.4.1\",\n    \"streamlit>=1.45.1\",\n    \"flask>=3.1.1\",\n    \"pdfplumber>=0.11.6\",\n    \"boto3>=1.38.31\",\n    \"pillow>=11.2.1\",\n    \"pymupdf>=1.26.0\",\n    \"pdf2image>=1.17.0\",\n    \"aiohttp>=3.11.18\",\n    \"openpyxl>=3.1.5\",\n]\n","size_bytes":442},"replit.md":{"content":"# PDF Text Extractor and Tabulator\n\n## Overview\n\nThis application extracts text from PDF files, processes the extracted content using an OpenAI language model to identify structured information, and displays the results in a tabulated format. Users can then download the results in various formats.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\nCommentary relevance: Only attach commentary to data fields if it's highly relevant and directly explains the specific data point.\n\n## System Architecture\n\nThe application follows a simple, streamlined architecture:\n\n1. **Frontend**: Streamlit web interface for user interactions\n2. **Backend Processing**: Python modules for PDF processing and LLM interaction \n3. **Data Flow**: PDF upload → Text extraction → LLM processing → Tabulation → Export options\n\nThe architecture prioritizes simplicity and straightforward user interaction, with each component handling a specific responsibility in the data processing pipeline.\n\n## Key Components\n\n### 1. Streamlit Interface (`app.py`)\nThe main entry point that provides:\n- PDF file upload functionality\n- Display of extracted and processed data\n- Export options for the processed data\n\n### 2. PDF Processing (`pdf_processor.py`)\nHandles the extraction of text from PDF files using PyPDF2:\n- `extract_text_from_pdf`: Processes PDF files from a file path\n- `extract_text_from_pdf_bytes`: Processes PDF content provided as bytes\n\n### 3. LLM Processing (`llm_processor.py`)\nLeverages OpenAI's GPT-4o model to:\n- Analyze extracted text from PDFs\n- Convert unstructured text into structured, tabular JSON data\n- Uses LangChain for prompt construction and model interaction\n\n### 4. Export Utilities (`export_utils.py`)\nProvides functionality to:\n- Export processed data to PDF format using ReportLab\n- Create downloadable links for the exported data\n\n## Data Flow\n\n1. **Input**: User uploads a PDF document through the Streamlit interface\n2. **Processing**:\n   - PDF text is extracted using PyPDF2\n   - Extracted text is sent to OpenAI's GPT-4o model\n   - The LLM analyzes the text and structures it into tabular format\n3. **Output**:\n   - Structured data is displayed as a table in the interface\n   - User can download the data in various formats\n\n## External Dependencies\n\n### Core Libraries\n- `streamlit`: Web application framework\n- `pandas`: Data manipulation and analysis\n- `PyPDF2`: PDF processing\n- `langchain`: LLM interaction framework\n- `openai`: API for GPT-4o access\n- `reportlab`: PDF generation for exports\n\n### External Services\n- **OpenAI API**: Requires an API key set as an environment variable (`OPENAI_API_KEY`)\n\n## Deployment Strategy\n\nThe application is configured for deployment on Replit with:\n- Python 3.11 runtime\n- Streamlit server running on port 5000\n- Autoscaling deployment target\n- Custom workflow configuration for the run button\n- Headless server mode with external accessibility\n\nThe deployment uses Nix packaging to ensure all dependencies are properly managed and the application runs consistently in the Replit environment.\n\n## Recent Changes\n\n### June 2025 - Enhanced Table Processing and Commentary System\n- **Multi-Column Table Reconstruction**: Enhanced prompts to preserve ALL columns from extracted tables instead of simplifying to 2 columns\n- **Document Text Tabulation**: Added comprehensive tabulation of document text content into structured table format\n- **Commentary Matching System**: Implemented intelligent matching between extracted data and document commentary\n- **Streaming Processing**: Added real-time streaming output for progressive data processing updates\n- **Enhanced Frontend Display**: Reconstructed tables now show original multi-column structure with proper headers and data rows\n- **Dual Table Views**: Both reconstructed original tables and detailed data point tables are displayed\n- **Document Content Tables**: Narrative text is now analyzed and tabulated into structured format alongside extracted tables\n- **Model Change**: Uses GPT-4o for all AI processing for highest quality results\n- **Commentary Summarization**: Added automatic summarization of long commentaries using GPT-4o\n- **Superscript Filtering**: Added filtering to ignore superscript numbers and footnote markers in data extraction\n- **Cost Calculator**: Implemented comprehensive cost tracking for all LLM calls with detailed token usage and dollar costs\n- **Dependency Cleanup**: Removed unnecessary libraries (crewai, langchain) that were making unauthorized OpenAI API calls\n\n### Architecture Improvements\n- **Advanced LLM Processing**: Updated prompts to extract comprehensive data while maintaining table structure integrity\n- **Asynchronous Commentary Matching**: Each data point is matched against document text to find relevant explanations\n- **Enhanced Data Structure**: Extended data model to include table headers, rows, commentary, and metadata\n- **Real-time Progress Updates**: Streaming endpoint provides live processing status updates\n- **Model Optimization**: All OpenAI API calls now use GPT-4o for highest quality results\n\n## Development Notes\n\n1. The application uses GPT-4o as the language model for highest quality processing\n2. Uses Amazon Textract for advanced PDF text and table extraction\n3. Export functionality supports JSON, CSV, and PDF formats\n4. All processed data is session-based and not persisted between sessions\n5. Commentary matching uses intelligent text analysis to relate document content to extracted data\n6. Commentary over 300 characters is automatically summarized to keep output concise\n7. Simple field-value table format for better readability\n8. Comprehensive cost tracking displays individual call costs and total session spend in real-time\n\n### December 2025 - Migration and Model Updates\n- **Replit Migration**: Successfully migrated from Replit Agent to standard Replit environment\n- **Security Enhancements**: Implemented proper API key management and secure credential storage\n- **Model Upgrade**: Switched all AI processing to use GPT-4o for highest quality results\n- **Performance Optimization**: Fixed commentary matching performance issues that were causing crashes\n- **Dependency Management**: Updated package management and workflow configuration for Replit compatibility","size_bytes":6296},"structured_llm_processor.py":{"content":"import json\nimport os\nfrom openai import OpenAI\nfrom typing import Dict, Any, List\nimport asyncio\nimport aiohttp\nimport concurrent.futures\n\n# Using gpt-4o-mini for optimal performance and cost efficiency\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nopenai_client = OpenAI(api_key=OPENAI_API_KEY)\n\n# GPT-4o-mini pricing per 1M tokens (as of 2024)\nGPT_4O_MINI_INPUT_COST = 0.150  # $0.150 per 1M input tokens\nGPT_4O_MINI_OUTPUT_COST = 0.600  # $0.600 per 1M output tokens\n\n\nclass CostTracker:\n\n    def __init__(self):\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n        self.total_cost = 0.0\n        self.api_calls = 0\n\n    def add_usage(self, input_tokens, output_tokens):\n        self.total_input_tokens += input_tokens\n        self.total_output_tokens += output_tokens\n        self.api_calls += 1\n\n        input_cost = (input_tokens / 1_000_000) * GPT_4O_MINI_INPUT_COST\n        output_cost = (output_tokens / 1_000_000) * GPT_4O_MINI_OUTPUT_COST\n        call_cost = input_cost + output_cost\n        self.total_cost += call_cost\n\n        return call_cost\n\n    def get_summary(self):\n        return {\n            'total_input_tokens':\n            self.total_input_tokens,\n            'total_output_tokens':\n            self.total_output_tokens,\n            'total_tokens':\n            self.total_input_tokens + self.total_output_tokens,\n            'total_cost_usd':\n            round(self.total_cost, 6),\n            'api_calls':\n            self.api_calls,\n            'input_cost_usd':\n            round(\n                (self.total_input_tokens / 1_000_000) * GPT_4O_MINI_INPUT_COST,\n                6),\n            'output_cost_usd':\n            round((self.total_output_tokens / 1_000_000) *\n                  GPT_4O_MINI_OUTPUT_COST, 6)\n        }\n\n\n# Global cost tracker instance\ncost_tracker = CostTracker()\n\n\ndef split_text_section(text_lines, max_lines=25):\n    \"\"\"Split text lines into manageable chunks with sentence boundary preservation\"\"\"\n    chunks = []\n    current_chunk = []\n\n    for i, line in enumerate(text_lines):\n        current_chunk.append(line)\n\n        # Check if we should create a chunk\n        if len(current_chunk) >= max_lines:\n            # Try to end at a sentence boundary\n            if line.strip().endswith(('.', '!', '?', ':')):\n                chunks.append(current_chunk)\n                current_chunk = []\n            elif len(\n                    current_chunk) >= max_lines + 5:  # Force split if too long\n                chunks.append(current_chunk)\n                current_chunk = []\n\n    # Add remaining lines\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n\n\nasync def process_table_data(table_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Process table data with GPT-4o-mini asynchronously - simple format\"\"\"\n    prompt = f\"\"\"Extract key data points from this table as simple field-value pairs.\n\nTable data:\n{json.dumps(table_data, indent=2)}\n\nInstructions:\n1. Extract important data points as field-value pairs\n2. Use clear, descriptive field names\n3. Focus on financial figures, dates, and key metrics\n4. Keep it simple and straightforward\n\nReturn JSON with field-value pairs:\n{{\n  \"Revenue\": \"value\",\n  \"Growth_Rate\": \"value\",\n  \"Date\": \"value\"\n}}\"\"\"\n\n    try:\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, lambda: openai_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }],\n                response_format={\"type\": \"json_object\"}))\n\n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {\"error\": \"No content received from OpenAI\"}\n\n        return {\n            \"page\": table_data.get(\"page\", 1),\n            \"structured_table\": result,\n            \"original_rows\": table_data.get(\"rows\", [])\n        }\n    except Exception as e:\n        print(f\"Error processing table: {e}\")\n        return {\n            \"page\": table_data.get(\"page\", 1),\n            \"structured_table\": {\n                \"error\": str(e)\n            },\n            \"original_rows\": table_data.get(\"rows\", [])\n        }\n\n\nasync def process_key_value_data(\n        key_value_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Process key-value pairs with GPT-4o-mini asynchronously\"\"\"\n    prompt = f\"\"\"You are a data extraction specialist. Below are key-value pairs extracted from a document.\n\nExtract and organize this information into clear field-value pairs. Focus on extracting actual data values like company names, dates, amounts, percentages, and other factual information.\n\nKey-Value pairs:\n{json.dumps(key_value_pairs, indent=2)}\n\nReturn a simple JSON object where each key is a descriptive field name and each value is the actual extracted data. Do not create nested structures or arrays. Provide the response as valid JSON format.\"\"\"\n\n    try:\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, lambda: openai_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }],\n                response_format={\"type\": \"json_object\"}))\n\n        # Track usage and cost\n        if hasattr(response, 'usage') and response.usage:\n            call_cost = cost_tracker.add_usage(\n                response.usage.prompt_tokens, response.usage.completion_tokens)\n            print(f\"Key-value processing cost: ${call_cost:.6f}\")\n\n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {\"error\": \"No content received from OpenAI\"}\n\n        return {\n            \"structured_key_values\": result,\n            \"original_pairs\": key_value_pairs\n        }\n    except Exception as e:\n        print(f\"Error processing key-value pairs: {e}\")\n        return {\n            \"structured_key_values\": {\n                \"error\": str(e)\n            },\n            \"original_pairs\": key_value_pairs\n        }\n\n\nasync def process_text_chunk(text_chunk: List[str]) -> Dict[str, Any]:\n    \"\"\"Process a text chunk with GPT-4o-mini asynchronously and tabulate the content\"\"\"\n    text_content = '\\n'.join(text_chunk)\n\n    prompt = f\"\"\"You are a financial document analyst. Extract and tabulate ALL meaningful data from this text segment.\n\nCreate a comprehensive table structure that captures the key information in a tabulated format.\n\nText:\n{text_content}\n\nRequirements:\n1. Extract ALL meaningful data points and organize them into a table structure\n2. Create appropriate column headers based on the content type\n3. Structure data into logical rows and columns\n4. Include financial metrics, dates, percentages, company info, etc.\n5. If the text contains narrative information, extract key facts and tabulate them\n6. IGNORE superscript numbers and footnote reference markers (¹²³ or (1)(2)(3) or [1][2][3])\n7. Extract clean data values without footnote symbols\n\nReturn JSON with BOTH table structure AND individual facts:\n{{\n  \"table_headers\": [\"Metric\", \"Value\", \"Period\", \"Context\"],\n  \"table_rows\": [\n    [\"Revenue\", \"$115.5M\", \"Q4 2023\", \"33% growth\"],\n    [\"MAU\", \"65.8M\", \"Q4 2023\", \"Global users\"],\n    [\"Market Share\", \"12%\", \"2023\", \"Primary market\"]\n  ],\n  \"extracted_facts\": {{\n    \"Company_Name\": \"Life360\",\n    \"Q4_Revenue\": \"$115.5 million\",\n    \"MAU_Growth\": \"33%\",\n    \"Market_Position\": \"Leading family safety platform\"\n  }}\n}}\n\nExtract comprehensive data - do not limit to just a few items. Return the response as valid JSON format.\"\"\"\n\n    try:\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, lambda: openai_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }],\n                response_format={\"type\": \"json_object\"}))\n\n        # Track usage and cost\n        if hasattr(response, 'usage') and response.usage:\n            call_cost = cost_tracker.add_usage(\n                response.usage.prompt_tokens, response.usage.completion_tokens)\n            print(f\"Text chunk processing cost: ${call_cost:.6f}\")\n\n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n        else:\n            result = {\"error\": \"No content received from OpenAI\"}\n\n        return {\n            \"table_headers\": result.get(\"table_headers\", []),\n            \"table_rows\": result.get(\"table_rows\", []),\n            \"extracted_facts\": result.get(\"extracted_facts\", {}),\n            \"original_text\": text_chunk\n        }\n    except Exception as e:\n        print(f\"Error processing text chunk: {e}\")\n        return {\n            \"extracted_facts\": {\n                \"error\": str(e)\n            },\n            \"original_text\": text_chunk\n        }\n\n\nasync def match_commentary_to_data(row_data: str,\n                                   text_chunks: List[str]) -> Dict[str, Any]:\n    \"\"\"Match document text commentary to table row data with strict relevance validation\"\"\"\n    text_content = '\\n'.join(text_chunks)\n\n    prompt = f\"\"\"You are a strict document analysis expert. Your job is to find ONLY highly relevant commentary that directly explains a specific data point.\n\nDATA POINT TO MATCH: {row_data}\n\nDOCUMENT TEXT TO SEARCH:\n{text_content}\n\nULTRA-STRICT MATCHING CRITERIA:\n1. The commentary MUST specifically mention the exact field name, value, or closely related terms\n2. The commentary MUST provide meaningful explanation, context, or analysis of THIS specific data point\n3. The commentary MUST be a complete sentence or paragraph that makes sense on its own\n4. REJECT any text that:\n   - Only mentions the topic generally without the specific value\n   - Starts mid-sentence or is incomplete\n   - Talks about different data points or unrelated information\n   - Is just a list item without explanation\n   - Contains only the value without context\n\nRELEVANCE SCORING:\n- Score 0-10 where 10 = perfect match with specific explanation\n- Only return commentary with score 8+ (highly relevant)\n- If best match scores below 8, return null\n\nReturn JSON:\n{{\"commentary\": \"complete relevant explanation\", \"relevant\": true, \"relevance_score\": 9}}\nOR  \n{{\"commentary\": null, \"relevant\": false, \"relevance_score\": 3}}\n\nBe extremely selective - better to return no commentary than irrelevant commentary.\"\"\"\n\n    try:\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, lambda: openai_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }],\n                response_format={\"type\": \"json_object\"}))\n\n        content = response.choices[0].message.content\n        if content:\n            result = json.loads(content)\n            return result\n        else:\n            return {\"commentary\": None, \"relevant\": False}\n\n    except Exception as e:\n        print(f\"Error matching commentary: {e}\")\n        return {\"commentary\": None, \"relevant\": False}\n\n\nasync def process_structured_data_with_llm_async(\n        structured_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Process all sections of structured data with asynchronous LLM calls\"\"\"\n\n    document_text = structured_data.get('document_text', [])\n    tables = structured_data.get('tables', [])\n    key_values = structured_data.get('key_values', [])\n\n    results = {\n        \"processed_tables\": [],\n        \"processed_key_values\": {},\n        \"processed_document_text\": [],\n        \"enhanced_data_with_commentary\": [],\n        \"general_commentary\": \"\",\n        \"summary\": {\n            \"total_tables\": len(tables),\n            \"total_key_values\": len(key_values),\n            \"total_text_lines\": len(document_text),\n            \"text_chunks_processed\": 0,\n            \"commentary_matches\": 0\n        }\n    }\n\n    # Create tasks for asynchronous processing\n    tasks = []\n\n    # Process tables asynchronously\n    if tables:\n        print(f\"Processing {len(tables)} tables asynchronously...\")\n        table_tasks = [process_table_data(table) for table in tables]\n        tasks.extend(table_tasks)\n\n    # Process key-value pairs\n    if key_values:\n        print(\n            f\"Processing {len(key_values)} key-value pairs asynchronously...\")\n        kv_task = process_key_value_data(key_values)\n        tasks.append(kv_task)\n\n    # Process document text in chunks\n    text_tasks = []\n    if document_text:\n        text_chunks = split_text_section(document_text, max_lines=20)\n        print(\n            f\"Processing document text in {len(text_chunks)} chunks asynchronously...\"\n        )\n        text_tasks = [process_text_chunk(chunk) for chunk in text_chunks]\n        tasks.extend(text_tasks)\n        results[\"summary\"][\"text_chunks_processed\"] = len(text_chunks)\n\n    # Execute all tasks concurrently\n    if tasks:\n        print(f\"Executing {len(tasks)} LLM processing tasks concurrently...\")\n        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Organize results\n        task_index = 0\n\n        # Process table results\n        if tables:\n            for i in range(len(tables)):\n                result = completed_tasks[task_index]\n                if isinstance(result, Exception):\n                    print(f\"Table processing error: {result}\")\n                    result = {\n                        \"error\": str(result),\n                        \"page\": tables[i].get(\"page\", 1)\n                    }\n                results[\"processed_tables\"].append(result)\n                task_index += 1\n\n        # Process key-value result\n        if key_values:\n            result = completed_tasks[task_index]\n            if isinstance(result, Exception):\n                print(f\"Key-value processing error: {result}\")\n                result = {\"error\": str(result)}\n            results[\"processed_key_values\"] = result\n            task_index += 1\n\n        # Process text chunk results\n        if text_tasks:\n            for i in range(len(text_tasks)):\n                result = completed_tasks[task_index]\n                if isinstance(result, Exception):\n                    print(f\"Text chunk processing error: {result}\")\n                    result = {\"error\": str(result)}\n                results[\"processed_document_text\"].append(result)\n                task_index += 1\n\n    # Phase 2: Enhanced data processing with commentary matching\n    print(\"Starting commentary matching phase...\")\n    await process_commentary_matching(results, document_text)\n\n    return results\n\n\nasync def process_commentary_matching(results: Dict[str, Any],\n                                      document_text: List[str]) -> None:\n    \"\"\"Process commentary matching for all extracted data with optimized performance\"\"\"\n    print(\"Starting optimized commentary matching phase...\")\n    \n    # Skip commentary matching if document is too large or has too many data points\n    total_data_points = 0\n    for table in results.get(\"processed_tables\", []):\n        total_data_points += len(table.get(\"structured_table\", {}).get(\"table_rows\", []))\n    \n    if results.get(\"processed_key_values\"):\n        total_data_points += len(results[\"processed_key_values\"].get(\"structured_key_values\", {}))\n    \n    for chunk in results.get(\"processed_document_text\", []):\n        total_data_points += len(chunk.get(\"extracted_facts\", {}))\n    \n    if total_data_points > 30 or len(document_text) > 150:\n        print(f\"Skipping commentary matching - too many items ({total_data_points} data points, {len(document_text)} text lines)\")\n        return\n    \n    # Process only the first few important data points sequentially\n    processed_count = 0\n    max_items = 8  # Limit total items processed\n    \n    # Process first table only\n    if results.get(\"processed_tables\") and processed_count < max_items:\n        table = results[\"processed_tables\"][0]\n        for i, row in enumerate(table.get(\"structured_table\", {}).get(\"table_rows\", [])):\n            if processed_count >= max_items or i >= 3:  # Max 3 rows per table\n                break\n            if isinstance(row, list) and len(row) >= 2:\n                data_point = f\"Field: {row[0]}, Value: {row[1]}\"\n                try:\n                    commentary_result = await match_commentary_to_data(data_point, document_text[:30])\n                    # Only add commentary if it's highly relevant (score 8+)\n                    if (commentary_result.get(\"relevant\") and \n                        commentary_result.get(\"commentary\") and \n                        commentary_result.get(\"relevance_score\", 0) >= 8):\n                        if \"commentary\" not in table:\n                            table[\"commentary\"] = {}\n                        table[\"commentary\"][f\"row_{i}\"] = commentary_result[\"commentary\"]\n                        print(f\"Added high-relevance commentary (score: {commentary_result.get('relevance_score')}) for {data_point}\")\n                    else:\n                        print(f\"Skipped low-relevance commentary (score: {commentary_result.get('relevance_score', 0)}) for {data_point}\")\n                    processed_count += 1\n                except Exception as e:\n                    print(f\"Error matching commentary for table row: {e}\")\n                    break\n    \n    print(f\"Commentary matching completed for {processed_count} items\")\n\n\ndef process_structured_data_with_llm(\n        structured_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Synchronous wrapper for asynchronous processing\"\"\"\n    return asyncio.run(process_structured_data_with_llm_async(structured_data))\n","size_bytes":17936},"textract_processor.py":{"content":"import boto3\nimport time\nimport uuid\nfrom typing import Dict, Any, List, Optional\n\nclass TextractProcessor:\n    def __init__(self):\n        \"\"\"Initialize AWS Textract and S3 clients with credentials from environment\"\"\"\n        self.textract_client = boto3.client('textract')\n        self.s3_client = boto3.client('s3')\n        self.bucket_name = 'textract-bucket-lk'\n\n    def extract_text_from_pdf_bytes(self, pdf_bytes: bytes) -> Dict[str, Any]:\n        \"\"\"\n        Extract structured data from PDF bytes using Amazon Textract with S3 storage.\n        \n        Args:\n            pdf_bytes (bytes): PDF file as bytes\n            \n        Returns:\n            Dict[str, Any]: Structured JSON with document_text, tables, and key_values\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            print(\"Using Amazon Textract with S3 storage for PDF processing\")\n            \n            # Upload PDF to S3\n            file_key = f\"textract-input/{uuid.uuid4()}.pdf\"\n            self.s3_client.put_object(\n                Bucket=self.bucket_name,\n                Key=file_key,\n                Body=pdf_bytes,\n                ContentType='application/pdf'\n            )\n            \n            # Start document analysis\n            response = self.textract_client.start_document_analysis(\n                DocumentLocation={\n                    'S3Object': {\n                        'Bucket': self.bucket_name,\n                        'Name': file_key\n                    }\n                },\n                FeatureTypes=['TABLES', 'FORMS']\n            )\n            \n            job_id = response['JobId']\n            print(f\"Started Textract job: {job_id}\")\n            \n            # Wait for job completion\n            while True:\n                result = self.textract_client.get_document_analysis(JobId=job_id)\n                status = result['JobStatus']\n                print(f\"Job status: {status}\")\n                \n                if status in ['SUCCEEDED', 'FAILED']:\n                    break\n                \n                time.sleep(5)\n            \n            if status == 'FAILED':\n                raise Exception(\"Textract job failed\")\n            \n            # Fetch full results (handle pagination)\n            pages = []\n            next_token = None\n            \n            while True:\n                if next_token:\n                    response = self.textract_client.get_document_analysis(JobId=job_id, NextToken=next_token)\n                else:\n                    response = self.textract_client.get_document_analysis(JobId=job_id)\n                \n                pages.extend(response['Blocks'])\n                \n                next_token = response.get('NextToken')\n                if not next_token:\n                    break\n            \n            print(f\"Total blocks extracted: {len(pages)}\")\n            \n            # Parse the results\n            structured_data = self._parse_textract_blocks(pages, start_time)\n            \n            # Clean up S3 file\n            try:\n                self.s3_client.delete_object(Bucket=self.bucket_name, Key=file_key)\n            except Exception as e:\n                print(f\"Warning: Could not delete S3 file: {e}\")\n            \n            return structured_data\n            \n        except Exception as e:\n            print(f\"Textract extraction failed: {e}\")\n            raise Exception(f\"Failed to extract text using Amazon Textract: {str(e)}\")\n\n    def _enhance_footnote_detection(self, document_text):\n        \"\"\"Enhanced footnote detection and processing\"\"\"\n        import re\n        \n        footnotes = []\n        enhanced_text = []\n        footnote_markers = {}\n        \n        # Common footnote patterns\n        footnote_patterns = [\n            r'^\\(\\d+\\)',  # (1), (2) at start of line\n            r'^\\[\\d+\\]',  # [1], [2] at start of line\n            r'^\\d+\\.',    # 1., 2., 3. at start of line\n            r'^\\*+\\s',    # *, **, *** at start with space\n            r'^Note\\s*\\d*:',  # Note: or Note 1:\n            r'^Source:',  # Source:\n            r'^See\\s',    # See ...\n        ]\n        \n        for i, line in enumerate(document_text):\n            line_stripped = line.strip()\n            if not line_stripped:\n                continue\n                \n            # Check if this line is a footnote\n            is_footnote = False\n            footnote_marker = None\n            \n            for pattern in footnote_patterns:\n                match = re.match(pattern, line_stripped, re.IGNORECASE)\n                if match:\n                    footnote_marker = match.group()\n                    # Additional checks for footnote characteristics\n                    if (len(line_stripped) > len(footnote_marker) + 5 and  # Has content after marker\n                        (any(word in line_stripped.lower() for word in \n                             ['note', 'source', 'see', 'reference', 'pursuant', 'accordance', \n                              'disclaimer', 'based on', 'refers to', 'includes', 'excludes']) or\n                         re.search(r'\\b(?:page|section|chapter|exhibit|appendix)\\s+\\d+', line_stripped.lower()))):\n                        is_footnote = True\n                        break\n            \n            if is_footnote:\n                footnotes.append({\n                    'marker': footnote_marker,\n                    'content': line_stripped,\n                    'line_number': i,\n                    'type': 'footnote'\n                })\n                footnote_markers[footnote_marker] = line_stripped\n            else:\n                # Check for inline footnote references\n                has_refs = bool(re.search(r'[\\(\\[]\\d+[\\)\\]]|\\*+(?=\\s|$)', line_stripped))\n                enhanced_text.append({\n                    'content': line_stripped,\n                    'has_footnote_refs': has_refs,\n                    'line_number': i\n                })\n        \n        return {\n            'enhanced_text': enhanced_text,\n            'footnotes': footnotes,\n            'footnote_markers': footnote_markers\n        }\n\n    def _remove_superscript_numbers(self, text):\n        \"\"\"Remove superscript numbers and common footnote markers from text\"\"\"\n        import re\n        \n        # Remove superscript numbers (Unicode superscript characters)\n        superscript_pattern = r'[⁰¹²³⁴⁵⁶⁷⁸⁹]+'\n        text = re.sub(superscript_pattern, '', text)\n        \n        # Remove common footnote reference patterns\n        footnote_patterns = [\n            r'\\(\\d+\\)',    # (1), (2), etc.\n            r'\\[\\d+\\]',    # [1], [2], etc.\n            r'\\*+',        # *, **, ***, etc.\n            r'^\\d+$',      # Standalone numbers on their own line\n        ]\n        \n        for pattern in footnote_patterns:\n            text = re.sub(pattern, '', text)\n        \n        # Clean up extra whitespace\n        text = ' '.join(text.split())\n        \n        return text.strip()\n\n    def _parse_textract_blocks(self, blocks: List[Dict[str, Any]], start_time: float) -> Dict[str, Any]:\n        \"\"\"Parse Textract blocks into the specified JSON format with enhanced footnote handling\"\"\"\n        \n        block_map = {block['Id']: block for block in blocks}\n        \n        # Extract document text (line by line)\n        raw_document_text = []\n        tables = []\n        key_values = []\n        \n        # Process blocks by page to maintain order\n        pages_blocks = {}\n        for block in blocks:\n            page_num = block.get('Page', 1)\n            if page_num not in pages_blocks:\n                pages_blocks[page_num] = []\n            pages_blocks[page_num].append(block)\n        \n        # Process each page in order\n        all_document_text = []\n        for page_num in sorted(pages_blocks.keys()):\n            page_blocks = pages_blocks[page_num]\n            \n            # Sort blocks by geometry (top to bottom, left to right)\n            line_blocks = [b for b in page_blocks if b['BlockType'] == 'LINE']\n            line_blocks.sort(key=lambda x: (\n                x.get('Geometry', {}).get('BoundingBox', {}).get('Top', 0),\n                x.get('Geometry', {}).get('BoundingBox', {}).get('Left', 0)\n            ))\n            \n            for block in line_blocks:\n                text = block.get('Text', '').strip()\n                if text:\n                    # Remove superscript numbers (common footnote references)\n                    cleaned_text = self._remove_superscript_numbers(text)\n                    if cleaned_text:\n                        all_document_text.append(cleaned_text)\n            \n            # Process other block types for this page\n            for block in page_blocks:\n                if block['BlockType'] == 'TABLE':\n                    table_data = self._extract_table_structure(block, block_map)\n                    if table_data:\n                        tables.append(table_data)\n                \n                elif block['BlockType'] == 'KEY_VALUE_SET':\n                    kv_pair = self._extract_key_value_pair(block, block_map)\n                    if kv_pair:\n                        key_values.append(kv_pair)\n        \n        # Enhanced footnote processing\n        footnote_analysis = self._enhance_footnote_detection(all_document_text)\n        \n        processing_time = f\"{time.time() - start_time:.1f}s\"\n        print(f\"Textract processing completed in {processing_time}\")\n        print(f\"Found {len(footnote_analysis['footnotes'])} footnotes\")\n        \n        return {\n            \"document_text\": all_document_text,\n            \"tables\": tables,\n            \"key_values\": key_values,\n            \"footnotes\": footnote_analysis['footnotes'],\n            \"footnote_markers\": footnote_analysis['footnote_markers'],\n            \"enhanced_text\": footnote_analysis['enhanced_text']\n        }\n\n    def _extract_table_structure(self, table_block: Dict[str, Any], block_map: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract table as rows format\"\"\"\n        if 'Relationships' not in table_block:\n            return None\n        \n        # Get page number\n        page_num = table_block.get('Page', 1)\n        \n        # Find all cells in the table\n        cells = []\n        for relationship in table_block['Relationships']:\n            if relationship['Type'] == 'CHILD':\n                for child_id in relationship['Ids']:\n                    if child_id in block_map and block_map[child_id]['BlockType'] == 'CELL':\n                        cells.append(block_map[child_id])\n        \n        if not cells:\n            return None\n        \n        # Organize cells by row and column\n        table_structure = {}\n        max_row = 0\n        max_col = 0\n        \n        for cell in cells:\n            row_index = cell.get('RowIndex', 1) - 1  # Convert to 0-based\n            col_index = cell.get('ColumnIndex', 1) - 1  # Convert to 0-based\n            \n            max_row = max(max_row, row_index)\n            max_col = max(max_col, col_index)\n            \n            cell_text = self._get_cell_text(cell, block_map)\n            \n            if row_index not in table_structure:\n                table_structure[row_index] = {}\n            table_structure[row_index][col_index] = cell_text\n        \n        # Convert to list of lists (rows)\n        rows = []\n        for row_idx in range(max_row + 1):\n            row = []\n            for col_idx in range(max_col + 1):\n                cell_value = table_structure.get(row_idx, {}).get(col_idx, \"\")\n                row.append(cell_value)\n            rows.append(row)\n        \n        return {\n            \"page\": page_num,\n            \"rows\": rows\n        }\n\n    def _get_cell_text(self, cell_block: Dict[str, Any], block_map: Dict[str, Any]) -> str:\n        \"\"\"Extract text from a table cell\"\"\"\n        if 'Relationships' not in cell_block:\n            return \"\"\n        \n        text_parts = []\n        for relationship in cell_block['Relationships']:\n            if relationship['Type'] == 'CHILD':\n                for child_id in relationship['Ids']:\n                    if child_id in block_map:\n                        child_block = block_map[child_id]\n                        if child_block['BlockType'] == 'WORD':\n                            text_parts.append(child_block.get('Text', ''))\n        \n        return ' '.join(text_parts)\n\n    def _extract_key_value_pair(self, kv_block: Dict[str, Any], block_map: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract key-value pairs\"\"\"\n        if kv_block.get('EntityTypes') and 'KEY' in kv_block['EntityTypes']:\n            page_num = kv_block.get('Page', 1)\n            \n            key_text = self._get_text_from_block(kv_block, block_map)\n            value_text = \"\"\n            \n            # Find the corresponding VALUE\n            if 'Relationships' in kv_block:\n                for relationship in kv_block['Relationships']:\n                    if relationship['Type'] == 'VALUE':\n                        for value_id in relationship['Ids']:\n                            if value_id in block_map:\n                                value_block = block_map[value_id]\n                                value_text = self._get_text_from_block(value_block, block_map)\n                                break\n            \n            if key_text:\n                return {\n                    \"key\": key_text,\n                    \"value\": value_text,\n                    \"page\": page_num\n                }\n        \n        return None\n\n    def _get_text_from_block(self, block: Dict[str, Any], block_map: Dict[str, Any]) -> str:\n        \"\"\"Get text content from a block\"\"\"\n        if 'Text' in block:\n            return block['Text']\n        \n        if 'Relationships' not in block:\n            return \"\"\n        \n        text_parts = []\n        for relationship in block['Relationships']:\n            if relationship['Type'] == 'CHILD':\n                for child_id in relationship['Ids']:\n                    if child_id in block_map:\n                        child_block = block_map[child_id]\n                        if child_block['BlockType'] in ['WORD', 'LINE']:\n                            text_parts.append(child_block.get('Text', ''))\n        \n        return ' '.join(text_parts)\n\n\ndef extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:\n    \"\"\"\n    Main function to extract raw text from PDF bytes using Amazon Textract.\n    Returns document_text joined for backward compatibility.\n    \n    Args:\n        pdf_bytes (bytes): PDF file as bytes\n        \n    Returns:\n        str: Raw extracted text from the PDF\n    \"\"\"\n    processor = TextractProcessor()\n    result = processor.extract_text_from_pdf_bytes(pdf_bytes)\n    return '\\n'.join(result.get('document_text', []))\n\n\ndef extract_structured_data_from_pdf_bytes(pdf_bytes: bytes) -> Dict[str, Any]:\n    \"\"\"\n    Main function to extract structured data from PDF bytes using Amazon Textract.\n    \n    Args:\n        pdf_bytes (bytes): PDF file as bytes\n        \n    Returns:\n        Dict[str, Any]: Structured JSON with document_text, tables, and key_values\n    \"\"\"\n    processor = TextractProcessor()\n    return processor.extract_text_from_pdf_bytes(pdf_bytes)\n\n\ndef extract_text_from_pdf(pdf_path: str) -> str:\n    \"\"\"\n    Main function to extract raw text from PDF file using Amazon Textract.\n    \n    Args:\n        pdf_path (str): Path to the PDF file\n        \n    Returns:\n        str: Raw extracted text from the PDF\n    \"\"\"\n    with open(pdf_path, 'rb') as file:\n        pdf_bytes = file.read()\n    return extract_text_from_pdf_bytes(pdf_bytes)","size_bytes":15551},"static/script.js":{"content":"// Main script for PDF Text Extractor and Tabulator\n\ndocument.addEventListener('DOMContentLoaded', function() {\n    // Elements\n    const dropArea = document.getElementById('drop-area');\n    const fileInput = document.getElementById('file-input');\n    const browseBtn = document.getElementById('browse-btn');\n    const fileInfo = document.getElementById('file-info');\n    const uploadProgress = document.getElementById('upload-progress');\n    const progressBar = uploadProgress.querySelector('.progress-bar');\n    const extractedTextSection = document.getElementById('extracted-text-section');\n    const extractedTextContent = document.getElementById('extracted-text-content');\n    const processBtn = document.getElementById('process-btn');\n    const resultsSection = document.getElementById('results-section');\n    const tableHeader = document.getElementById('table-header');\n    const tableBody = document.getElementById('table-body');\n    const exportJsonBtn = document.getElementById('export-json-btn');\n    const exportCsvBtn = document.getElementById('export-csv-btn');\n    const exportPdfBtn = document.getElementById('export-pdf-btn');\n    const loadingOverlay = document.getElementById('loading-overlay');\n    const loadingMessage = document.getElementById('loading-message');\n    const errorMessage = document.getElementById('error-message');\n\n    // Store data\n    let extractedText = '';\n    let processedData = [];\n    let currentStructuredData = null;\n    let csvData = '';  // Store CSV data for XLSX export\n\n    // Initialize drag and drop events\n    ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n        dropArea.addEventListener(eventName, preventDefaults, false);\n    });\n\n    function preventDefaults(e) {\n        e.preventDefault();\n        e.stopPropagation();\n    }\n\n    ['dragenter', 'dragover'].forEach(eventName => {\n        dropArea.addEventListener(eventName, highlight, false);\n    });\n\n    ['dragleave', 'drop'].forEach(eventName => {\n        dropArea.addEventListener(eventName, unhighlight, false);\n    });\n\n    function highlight() {\n        dropArea.classList.add('dragover');\n    }\n\n    function unhighlight() {\n        dropArea.classList.remove('dragover');\n    }\n\n    // Handle file drop\n    dropArea.addEventListener('drop', handleDrop, false);\n\n    function handleDrop(e) {\n        const dt = e.dataTransfer;\n        const files = dt.files;\n        \n        if (files.length) {\n            handleFiles(files);\n        }\n    }\n\n    // Handle file selection via browse button\n    browseBtn.addEventListener('click', () => {\n        fileInput.click();\n    });\n\n    fileInput.addEventListener('change', () => {\n        if (fileInput.files.length) {\n            handleFiles(fileInput.files);\n        }\n    });\n\n    function handleFiles(files) {\n        const file = files[0];\n        \n        if (!file.type.includes('pdf')) {\n            showError('Please upload a PDF file');\n            return;\n        }\n        \n        showFileInfo(file);\n        uploadFile(file);\n    }\n\n    function showFileInfo(file) {\n        fileInfo.textContent = `File: ${file.name} (${formatFileSize(file.size)})`;\n        fileInfo.classList.remove('d-none');\n    }\n\n    function formatFileSize(bytes) {\n        if (bytes < 1024) return bytes + ' bytes';\n        else if (bytes < 1048576) return (bytes / 1024).toFixed(1) + ' KB';\n        else return (bytes / 1048576).toFixed(1) + ' MB';\n    }\n\n    function uploadFile(file) {\n        showLoading('Extracting text from PDF...');\n        \n        // Set up form data\n        const formData = new FormData();\n        formData.append('pdf', file);\n\n        // Show progress\n        uploadProgress.classList.remove('d-none');\n        progressBar.style.width = '0%';\n        \n        // Simulate progress (since fetch doesn't provide upload progress easily)\n        let progress = 0;\n        const progressInterval = setInterval(() => {\n            progress += 5;\n            if (progress <= 90) {\n                progressBar.style.width = progress + '%';\n            }\n            \n            if (progress > 90) {\n                clearInterval(progressInterval);\n            }\n        }, 100);\n\n        // Send file to server\n        fetch('/extract', {\n            method: 'POST',\n            body: formData\n        })\n        .then(response => {\n            clearInterval(progressInterval);\n            progressBar.style.width = '100%';\n            \n            if (!response.ok) {\n                return response.json().then(data => {\n                    throw new Error(data.error || 'Failed to extract text');\n                });\n            }\n            return response.json();\n        })\n        .then(data => {\n            hideLoading();\n            \n            if (data.document_text) {\n                extractedText = data.document_text.join('\\n');\n                currentStructuredData = data;\n                showExtractedText(extractedText, data);\n            } else {\n                throw new Error('No text was extracted from the PDF');\n            }\n        })\n        .catch(error => {\n            hideLoading();\n            showError(error.message);\n        });\n    }\n\n    function showExtractedText(text, structuredData = null) {\n        // Clear any existing structured info\n        const existingStructuredInfo = extractedTextSection.querySelector('.structured-data-info');\n        if (existingStructuredInfo) {\n            existingStructuredInfo.remove();\n        }\n        \n        // Show processing info and skip to AI processing\n        if (structuredData) {\n            const documentText = structuredData.document_text || [];\n            const tables = structuredData.tables || [];\n            const keyValues = structuredData.key_values || [];\n            \n            const structuredInfo = document.createElement('div');\n            structuredInfo.className = 'alert alert-success mb-3 structured-data-info';\n            structuredInfo.innerHTML = `\n                <h6 class=\"mb-2\">📊 Document Processed Successfully</h6>\n                <div class=\"row small\">\n                    <div class=\"col-md-3\">Text Lines: ${documentText.length}</div>\n                    <div class=\"col-md-3\">Tables: ${tables.length}</div>\n                    <div class=\"col-md-3\">Key-Values: ${keyValues.length}</div>\n                    <div class=\"col-md-3\">Processing: Complete</div>\n                </div>\n                <div class=\"text-center mt-3\">\n                    <button id=\"process-ai-btn\" class=\"btn btn-primary btn-lg\">\n                        <i class=\"bi bi-gear\"></i> Process with AI\n                    </button>\n                    <button class=\"btn btn-outline-secondary btn-sm ms-2\" onclick=\"showJsonModal()\">\n                        View Raw JSON\n                    </button>\n                </div>\n            `;\n            \n            // Replace extracted text section content\n            extractedTextSection.innerHTML = '';\n            extractedTextSection.appendChild(structuredInfo);\n            extractedTextSection.classList.remove('d-none');\n            \n            // Add event listener for AI processing\n            document.getElementById('process-ai-btn').addEventListener('click', processText);\n        }\n        \n        window.scrollTo({\n            top: extractedTextSection.offsetTop - 20,\n            behavior: 'smooth'\n        });\n    }\n\n    // Process extracted text\n    processBtn.addEventListener('click', processText);\n\n    function processText() {\n        if (!currentStructuredData) {\n            showError('No structured data to process');\n            return;\n        }\n\n        showLoading('Starting AI processing...');\n\n        // Try streaming first, fallback to regular processing\n        processWithStreaming();\n    }\n\n    function processWithStreaming() {\n        fetch('/process_stream', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify(currentStructuredData)\n        })\n        .then(response => {\n            if (!response.ok) {\n                throw new Error('Streaming not available');\n            }\n            \n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n            let buffer = '';\n            \n            function readStream() {\n                return reader.read().then(({ done, value }) => {\n                    if (done) {\n                        hideLoading();\n                        return;\n                    }\n                    \n                    buffer += decoder.decode(value, { stream: true });\n                    const lines = buffer.split('\\n');\n                    \n                    // Keep the last incomplete line in buffer\n                    buffer = lines.pop() || '';\n                    \n                    lines.forEach(line => {\n                        if (line.trim().startsWith('data: ')) {\n                            try {\n                                const jsonStr = line.slice(6).trim();\n                                if (jsonStr && jsonStr !== '{}') {\n                                    const data = JSON.parse(jsonStr);\n                                    handleStreamingData(data);\n                                }\n                            } catch (e) {\n                                console.error('Error parsing streaming data:', e, 'Line:', line);\n                            }\n                        }\n                    });\n                    \n                    return readStream();\n                });\n            }\n            \n            return readStream();\n        })\n        .catch(error => {\n            console.log('Streaming failed, using regular processing');\n            processRegular();\n        });\n    }\n\n    let streamedRows = [];\n    let streamedCSVRows = [];  // Store raw CSV rows for reconstruction\n    let tableInitialized = false;\n\n    function handleStreamingData(data) {\n        if (data.type === 'header') {\n            // Handle CSV header\n            streamedCSVRows = [data.content];  // Initialize with header\n            if (!tableInitialized) {\n                initializeStreamingTable();\n                tableInitialized = true;\n            }\n        } else if (data.type === 'row') {\n            // Handle CSV row content\n            streamedCSVRows.push(data.content);  // Store raw CSV row\n            displayStreamingCSVRow(data.content);\n        } else if (data.type === 'complete') {\n            hideLoading();\n            console.log('Streaming complete. Total rows:', data.total_rows);\n            // Reconstruct CSV data from streamed rows\n            csvData = streamedCSVRows.join('\\n');\n            finalizeStreamingDisplay(data.cost_summary);\n        } else if (data.status === 'error') {\n            hideLoading();\n            showError('Processing failed: ' + data.error);\n        }\n    }\n\n    function displayStreamingCSVRow(csvRow) {\n        if (!tableInitialized) {\n            initializeStreamingTable();\n            tableInitialized = true;\n        }\n        \n        // Parse CSV row - extract content after \"rowN: \"\n        const colonIndex = csvRow.indexOf(': ');\n        if (colonIndex === -1) return;\n        \n        const csvContent = csvRow.substring(colonIndex + 2);\n        \n        // Simple CSV parsing (handle quoted values)\n        const values = parseCSVRow(csvContent);\n        if (values.length < 6) return;  // Ensure we have all 6 columns\n        \n        const tableBody = document.getElementById('streaming-table-body');\n        const tr = document.createElement('tr');\n        \n        tr.innerHTML = `\n            <td><span class=\"badge bg-secondary\">${values[0]}</span></td>\n            <td><span class=\"badge bg-info\">${values[1]}</span></td>\n            <td><strong>${values[2]}</strong></td>\n            <td>${values[3]}</td>\n            <td>${values[4]}</td>\n            <td class=\"commentary-cell\">${values[5] || '<span class=\"text-muted\">-</span>'}</td>\n        `;\n        \n        if (values[5] && values[5].trim()) {\n            tr.classList.add('has-commentary');\n        }\n        \n        tableBody.appendChild(tr);\n        streamedRows.push(values);\n    }\n    \n    function parseCSVRow(csvString) {\n        const values = [];\n        let current = '';\n        let inQuotes = false;\n        \n        for (let i = 0; i < csvString.length; i++) {\n            const char = csvString[i];\n            \n            if (char === '\"') {\n                if (inQuotes && csvString[i + 1] === '\"') {\n                    current += '\"';  // Escaped quote\n                    i++;  // Skip next quote\n                } else {\n                    inQuotes = !inQuotes;\n                }\n            } else if (char === ',' && !inQuotes) {\n                values.push(current.trim());\n                current = '';\n            } else {\n                current += char;\n            }\n        }\n        \n        values.push(current.trim());  // Add last value\n        return values;\n    }\n\n    function updateStreamingRow(rowData) {\n        const rowIndex = streamedRows.findIndex(row => \n            row.field === rowData.field && row.source === rowData.source\n        );\n        \n        if (rowIndex >= 0) {\n            streamedRows[rowIndex] = rowData;\n            const tr = document.getElementById(`row-${rowIndex}`);\n            if (tr) {\n                const commentaryCell = tr.querySelector('.commentary-cell');\n                if (commentaryCell) {\n                    commentaryCell.innerHTML = rowData.commentary ? \n                        `<span class=\"text-muted small\">${rowData.commentary}</span>` : \n                        '<span class=\"text-muted\">-</span>';\n                }\n                if (rowData.commentary) {\n                    tr.classList.add('has-commentary');\n                }\n            }\n        }\n    }\n\n    function initializeStreamingTable() {\n        hideLoading();\n        resultsSection.innerHTML = `\n            <h4>Extracted Data with Commentary</h4>\n            <div class=\"table-responsive\">\n                <table class=\"table table-striped table-hover\">\n                    <thead class=\"table-dark\">\n                        <tr>\n                            <th>Source</th>\n                            <th>Type</th>\n                            <th>Field</th>\n                            <th>Value</th>\n                            <th>Page</th>\n                            <th>Commentary</th>\n                        </tr>\n                    </thead>\n                    <tbody id=\"streaming-table-body\"></tbody>\n                </table>\n            </div>\n            <div class=\"mt-3\">\n                <button class=\"btn btn-primary\" id=\"export-xlsx-btn\"><i class=\"bi bi-file-earmark-excel\"></i> Export as XLSX</button>\n                <button class=\"btn btn-outline-success\" id=\"export-csv-btn\">Export as CSV</button>\n                <button class=\"btn btn-outline-primary\" id=\"export-json-btn\">Export as JSON</button>\n                <button class=\"btn btn-outline-danger\" id=\"export-pdf-btn\">Export as PDF</button>\n            </div>\n        `;\n        \n        resultsSection.classList.remove('d-none');\n    }\n\n    function finalizeStreamingDisplay(costSummary = null) {\n        // Re-attach export event listeners\n        document.getElementById('export-xlsx-btn').addEventListener('click', exportXlsx);\n        document.getElementById('export-csv-btn').addEventListener('click', exportCsv);\n        document.getElementById('export-json-btn').addEventListener('click', exportJson);\n        document.getElementById('export-pdf-btn').addEventListener('click', exportPdf);\n        \n        // Add summary with cost information\n        const summaryDiv = document.createElement('div');\n        summaryDiv.className = 'alert alert-success mt-3';\n        \n        let costInfo = '';\n        if (costSummary && costSummary.total_cost_usd) {\n            costInfo = ` | LLM Cost: $${costSummary.total_cost_usd.toFixed(6)} (${costSummary.total_tokens.toLocaleString()} tokens, ${costSummary.api_calls} API calls)`;\n        }\n        \n        summaryDiv.innerHTML = `\n            <h6>Processing Complete</h6>\n            <small>Total rows extracted: ${streamedRows.length} | Financial data in XLSX format${costInfo}</small>\n        `;\n        resultsSection.appendChild(summaryDiv);\n    }\n\n    function displayProcessingSummary(data) {\n        resultsSection.innerHTML = `\n            <div class=\"alert alert-info\">\n                <h5>Processing Complete</h5>\n                <p>The document was analyzed successfully but no structured data could be extracted in the expected format.</p>\n                <div class=\"mt-3\">\n                    <h6>Processing Details:</h6>\n                    <ul>\n                        <li>Tables processed: ${data.summary?.total_tables || 0}</li>\n                        <li>Key-value pairs: ${data.summary?.total_key_values || 0}</li>\n                        <li>Text chunks: ${data.summary?.text_chunks_processed || 0}</li>\n                        <li>Commentary matches: ${data.summary?.commentary_matches || 0}</li>\n                    </ul>\n                </div>\n                ${data.processed_tables ? `\n                    <div class=\"mt-3\">\n                        <h6>Raw Processing Results:</h6>\n                        <pre class=\"bg-light p-3 small\">${JSON.stringify(data.processed_tables, null, 2)}</pre>\n                    </div>\n                ` : ''}\n            </div>\n        `;\n        resultsSection.classList.remove('d-none');\n    }\n\n    function processRegular() {\n        showLoading('Processing structured data with AI...');\n\n        fetch('/process', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify(currentStructuredData)\n        })\n        .then(response => {\n            if (!response.ok) {\n                return response.json().then(data => {\n                    throw new Error(data.error || 'Failed to process text');\n                });\n            }\n            return response.json();\n        })\n        .then(data => {\n            hideLoading();\n            \n            processedData = data.dataframe_data || [];\n            displayResultsWithTables(processedData);\n            \n            if (data.summary) {\n                showProcessingSummary(data.summary);\n            }\n        })\n        .catch(error => {\n            hideLoading();\n            showError(error.message);\n        });\n    }\n\n    function displayStructuredResults(data) {\n        const resultsSection = document.getElementById('results-section');\n        \n        // Clear previous results\n        resultsSection.innerHTML = '';\n        \n        let html = `\n            <div class=\"processing-summary alert alert-success mb-3\">\n                <h5>AI Processing Complete</h5>\n                <div class=\"row small\">\n                    <div class=\"col-md-3\">Tables: ${data.processed_tables?.length || 0}</div>\n                    <div class=\"col-md-3\">Key-Values: ${data.processed_key_values ? 'Processed' : 'None'}</div>\n                    <div class=\"col-md-3\">Text Chunks: ${data.processed_document_text?.length || 0}</div>\n                    <div class=\"col-md-3\">Total Rows: ${data.total_rows || 0}</div>\n                </div>\n            </div>\n        `;\n        \n        let allCsvData = [];\n        let hasData = false;\n        \n        // Display Tables\n        if (data.processed_tables && data.processed_tables.length > 0) {\n            html += '<h5>📊 Extracted Tables</h5>';\n            data.processed_tables.forEach((table, index) => {\n                if (table.structured_table && !table.structured_table.error) {\n                    hasData = true;\n                    html += `\n                        <div class=\"card mb-3\">\n                            <div class=\"card-header bg-primary text-white\">\n                                <h6 class=\"mb-0\">Table ${index + 1} (Page ${table.page})</h6>\n                            </div>\n                            <div class=\"card-body\">\n                                <div class=\"table-responsive\">\n                                    <table class=\"table table-striped table-sm\">\n                                        <thead class=\"table-light\">\n                                            <tr><th>Field</th><th>Value</th></tr>\n                                        </thead>\n                                        <tbody>\n                    `;\n                    \n                    Object.entries(table.structured_table).forEach(([key, value]) => {\n                        if (key !== 'error') {\n                            html += `<tr><td><strong>${key}</strong></td><td>${value}</td></tr>`;\n                            allCsvData.push({\n                                source: `Table ${index + 1}`,\n                                type: 'Table Data',\n                                field: key,\n                                value: String(value),\n                                page: table.page\n                            });\n                        }\n                    });\n                    \n                    html += `\n                                        </tbody>\n                                    </table>\n                                </div>\n                            </div>\n                        </div>\n                    `;\n                }\n            });\n        }\n        \n        // Display Key-Value Pairs\n        if (data.processed_key_values && data.processed_key_values.structured_key_values && !data.processed_key_values.structured_key_values.error) {\n            hasData = true;\n            html += `\n                <h5>🔑 Key-Value Pairs</h5>\n                <div class=\"card mb-3\">\n                    <div class=\"card-header bg-info text-white\">\n                        <h6 class=\"mb-0\">Document Metadata</h6>\n                    </div>\n                    <div class=\"card-body\">\n                        <div class=\"table-responsive\">\n                            <table class=\"table table-striped table-sm\">\n                                <thead class=\"table-light\">\n                                    <tr><th>Field</th><th>Value</th></tr>\n                                </thead>\n                                <tbody>\n            `;\n            \n            Object.entries(data.processed_key_values.structured_key_values).forEach(([key, value]) => {\n                if (key !== 'error') {\n                    html += `<tr><td><strong>${key}</strong></td><td>${value}</td></tr>`;\n                    allCsvData.push({\n                        source: 'Key-Value Pairs',\n                        type: 'Structured Data',\n                        field: key,\n                        value: String(value),\n                        page: 'N/A'\n                    });\n                }\n            });\n            \n            html += `\n                                </tbody>\n                            </table>\n                        </div>\n                    </div>\n                </div>\n            `;\n        }\n        \n        // Display Financial Data\n        if (data.processed_document_text && data.processed_document_text.length > 0) {\n            const validChunks = data.processed_document_text.filter(chunk => \n                chunk.extracted_facts && !chunk.extracted_facts.error && \n                Object.keys(chunk.extracted_facts).length > 0\n            );\n            \n            if (validChunks.length > 0) {\n                hasData = true;\n                html += '<h5>💰 Financial & Business Data</h5>';\n                \n                validChunks.forEach((chunk, index) => {\n                    html += `\n                        <div class=\"card mb-3\">\n                            <div class=\"card-header bg-success text-white\">\n                                <h6 class=\"mb-0\">Text Segment ${index + 1}</h6>\n                            </div>\n                            <div class=\"card-body\">\n                                <div class=\"table-responsive\">\n                                    <table class=\"table table-striped table-sm\">\n                                        <thead class=\"table-light\">\n                                            <tr><th>Metric</th><th>Value</th></tr>\n                                        </thead>\n                                        <tbody>\n                    `;\n                    \n                    Object.entries(chunk.extracted_facts).forEach(([key, value]) => {\n                        if (key !== 'error') {\n                            html += `<tr><td><strong>${key}</strong></td><td>${value}</td></tr>`;\n                            allCsvData.push({\n                                source: `Text Segment ${index + 1}`,\n                                type: 'Financial Data',\n                                field: key,\n                                value: String(value),\n                                page: 'N/A'\n                            });\n                        }\n                    });\n                    \n                    html += `\n                                        </tbody>\n                                    </table>\n                                </div>\n                            </div>\n                        </div>\n                    `;\n                });\n            }\n        }\n        \n        if (!hasData) {\n            html += `\n                <div class=\"alert alert-warning\">\n                    <h6>No structured data found</h6>\n                    <p>The AI processing did not extract meaningful data from the document.</p>\n                </div>\n            `;\n        }\n        \n        // Add export buttons\n        html += `\n            <div class=\"text-center mt-4\">\n                <button id=\"export-csv-btn\" class=\"btn btn-success me-2\" ${!hasData ? 'disabled' : ''}>\n                    <i class=\"bi bi-file-earmark-spreadsheet\"></i> Export CSV (${allCsvData.length} rows)\n                </button>\n                <button id=\"export-json-btn\" class=\"btn btn-outline-primary me-2\">\n                    <i class=\"bi bi-file-earmark-code\"></i> Export JSON\n                </button>\n                <button id=\"export-excel-btn\" class=\"btn btn-outline-success\" ${!hasData ? 'disabled' : ''}>\n                    <i class=\"bi bi-file-earmark-excel\"></i> Export Excel\n                </button>\n            </div>\n        `;\n        \n        resultsSection.innerHTML = html;\n        resultsSection.classList.remove('d-none');\n        \n        // Add export event listeners\n        if (hasData) {\n            document.getElementById('export-csv-btn').addEventListener('click', () => {\n                exportToCSV(allCsvData);\n            });\n            \n            document.getElementById('export-excel-btn').addEventListener('click', () => {\n                exportToExcel(allCsvData);\n            });\n        }\n        \n        document.getElementById('export-json-btn').addEventListener('click', () => {\n            const blob = new Blob([JSON.stringify(data, null, 2)], { type: 'application/json' });\n            const url = URL.createObjectURL(blob);\n            const a = document.createElement('a');\n            a.href = url;\n            a.download = 'structured_data.json';\n            document.body.appendChild(a);\n            a.click();\n            document.body.removeChild(a);\n            URL.revokeObjectURL(url);\n        });\n        \n        window.scrollTo({\n            top: resultsSection.offsetTop - 20,\n            behavior: 'smooth'\n        });\n    }\n    \n    function createUnifiedTable(data) {\n        let unifiedData = [];\n        \n        // Helper function to extract values from nested objects\n        function extractValues(obj, source, type, page = null) {\n            if (!obj || typeof obj !== 'object') return [];\n            \n            let results = [];\n            \n            // Handle different data structures\n            if (Array.isArray(obj)) {\n                obj.forEach((item, index) => {\n                    if (typeof item === 'object' && item !== null) {\n                        // Recursively extract from array items\n                        const subResults = extractValues(item, source, type, page);\n                        results = results.concat(subResults);\n                    } else if (item !== null && item !== undefined && item !== '') {\n                        results.push({\n                            source: source,\n                            type: type,\n                            field: `item_${index}`,\n                            value: String(item),\n                            page: page\n                        });\n                    }\n                });\n            } else {\n                // Handle object with nested properties\n                function flattenObject(obj, prefix = '') {\n                    Object.entries(obj).forEach(([key, value]) => {\n                        if (value !== null && value !== undefined && value !== '') {\n                            const fieldName = prefix ? `${prefix}.${key}` : key;\n                            \n                            if (Array.isArray(value)) {\n                                // Handle arrays by extracting each element\n                                value.forEach((arrayItem, arrayIndex) => {\n                                    if (typeof arrayItem === 'object' && arrayItem !== null) {\n                                        // Recursively flatten array objects\n                                        const subResults = extractValues(arrayItem, source, type, page);\n                                        results = results.concat(subResults.map(r => ({\n                                            ...r,\n                                            field: `${fieldName}[${arrayIndex}].${r.field}`\n                                        })));\n                                    } else if (arrayItem !== null && arrayItem !== undefined && arrayItem !== '') {\n                                        results.push({\n                                            source: source,\n                                            type: type,\n                                            field: `${fieldName}[${arrayIndex}]`,\n                                            value: String(arrayItem),\n                                            page: page\n                                        });\n                                    }\n                                });\n                            } else if (typeof value === 'object' && value !== null) {\n                                // Recursively flatten nested objects\n                                flattenObject(value, fieldName);\n                            } else {\n                                results.push({\n                                    source: source,\n                                    type: type,\n                                    field: fieldName,\n                                    value: String(value),\n                                    page: page\n                                });\n                            }\n                        }\n                    });\n                }\n                \n                flattenObject(obj);\n            }\n            \n            return results;\n        }\n        \n        // Process tables\n        if (data.processed_tables && data.processed_tables.length > 0) {\n            data.processed_tables.forEach((table, index) => {\n                if (table.structured_table && !table.structured_table.error) {\n                    const tableResults = extractValues(\n                        table.structured_table, \n                        `Table ${index + 1}`, \n                        'Table Data', \n                        table.page\n                    );\n                    unifiedData = unifiedData.concat(tableResults);\n                }\n            });\n        }\n        \n        // Process key-value pairs\n        if (data.processed_key_values && data.processed_key_values.structured_key_values && !data.processed_key_values.structured_key_values.error) {\n            const kvResults = extractValues(\n                data.processed_key_values.structured_key_values,\n                'Key-Value Pairs',\n                'Structured Data'\n            );\n            unifiedData = unifiedData.concat(kvResults);\n        }\n        \n        // Process extracted facts from document text\n        if (data.processed_document_text && data.processed_document_text.length > 0) {\n            data.processed_document_text.forEach((chunk, chunkIndex) => {\n                if (chunk.extracted_facts && !chunk.extracted_facts.error) {\n                    const factsResults = extractValues(\n                        chunk.extracted_facts,\n                        `Text Chunk ${chunkIndex + 1}`,\n                        'Financial Data'\n                    );\n                    unifiedData = unifiedData.concat(factsResults);\n                }\n            });\n        }\n        \n        return unifiedData;\n    }\n    \n    function convertTableToHTML(tableData, tableIndex, page) {\n        let html = `\n            <div class=\"table-responsive mb-4\">\n                <table class=\"table table-striped table-hover\">\n                    <thead class=\"table-dark\">\n        `;\n        \n        let csvData = [];\n        let headers = [];\n        \n        // Helper function to safely convert values to string\n        function safeStringify(value) {\n            if (value === null || value === undefined) return '';\n            if (typeof value === 'object') {\n                return JSON.stringify(value);\n            }\n            return String(value);\n        }\n        \n        // Handle different table structures\n        if (Array.isArray(tableData)) {\n            // Array of objects\n            if (tableData.length > 0 && typeof tableData[0] === 'object') {\n                headers = Object.keys(tableData[0]);\n                html += '<tr>';\n                headers.forEach(header => {\n                    html += `<th>${header}</th>`;\n                });\n                html += '</tr></thead><tbody>';\n                \n                tableData.forEach(row => {\n                    html += '<tr>';\n                    let csvRow = { source: `Table ${tableIndex} (Page ${page})`, type: 'Table Data' };\n                    headers.forEach(header => {\n                        const value = safeStringify(row[header]);\n                        html += `<td>${value}</td>`;\n                        csvRow[header] = value;\n                    });\n                    html += '</tr>';\n                    csvData.push(csvRow);\n                });\n            }\n        } else if (typeof tableData === 'object' && tableData !== null) {\n            // Object with key-value pairs\n            headers = ['Field', 'Value'];\n            html += '<tr><th>Field</th><th>Value</th></tr></thead><tbody>';\n            \n            // Flatten nested objects for better display\n            function flattenObject(obj, prefix = '') {\n                let flattened = {};\n                for (let key in obj) {\n                    if (obj.hasOwnProperty(key)) {\n                        const fullKey = prefix ? `${prefix}.${key}` : key;\n                        const value = obj[key];\n                        \n                        if (typeof value === 'object' && value !== null && !Array.isArray(value)) {\n                            // Recursively flatten nested objects\n                            Object.assign(flattened, flattenObject(value, fullKey));\n                        } else {\n                            flattened[fullKey] = safeStringify(value);\n                        }\n                    }\n                }\n                return flattened;\n            }\n            \n            const flattenedData = flattenObject(tableData);\n            \n            Object.entries(flattenedData).forEach(([key, value]) => {\n                html += `<tr><td><strong>${key}</strong></td><td>${value}</td></tr>`;\n                csvData.push({\n                    source: `Table ${tableIndex} (Page ${page})`,\n                    type: 'Table Data',\n                    field: key,\n                    value: value\n                });\n            });\n        }\n        \n        html += '</tbody></table></div>';\n        \n        return { html, csvData };\n    }\n    \n    function convertKeyValuesToHTML(kvData) {\n        let html = `\n            <div class=\"table-responsive mb-4\">\n                <table class=\"table table-striped\">\n                    <thead class=\"table-dark\">\n                        <tr><th>Field</th><th>Value</th></tr>\n                    </thead>\n                    <tbody>\n        `;\n        \n        let csvData = [];\n        \n        // Helper function to safely convert values to string\n        function safeStringify(value) {\n            if (value === null || value === undefined) return '';\n            if (typeof value === 'object') {\n                return JSON.stringify(value);\n            }\n            return String(value);\n        }\n        \n        // Flatten nested objects for better display\n        function flattenObject(obj, prefix = '') {\n            let flattened = {};\n            for (let key in obj) {\n                if (obj.hasOwnProperty(key)) {\n                    const fullKey = prefix ? `${prefix}.${key}` : key;\n                    const value = obj[key];\n                    \n                    if (typeof value === 'object' && value !== null && !Array.isArray(value)) {\n                        Object.assign(flattened, flattenObject(value, fullKey));\n                    } else {\n                        flattened[fullKey] = safeStringify(value);\n                    }\n                }\n            }\n            return flattened;\n        }\n        \n        const flattenedData = flattenObject(kvData);\n        \n        Object.entries(flattenedData).forEach(([key, value]) => {\n            html += `<tr><td><strong>${key}</strong></td><td>${value}</td></tr>`;\n            csvData.push({\n                source: 'Key-Value Pairs',\n                type: 'Structured Data',\n                field: key,\n                value: value\n            });\n        });\n        \n        html += '</tbody></table></div>';\n        \n        return { html, csvData };\n    }\n    \n    function convertFactsToHTML(factsArray) {\n        let html = `\n            <div class=\"table-responsive mb-4\">\n                <table class=\"table table-striped\">\n                    <thead class=\"table-dark\">\n                        <tr><th>Metric</th><th>Value</th><th>Source</th></tr>\n                    </thead>\n                    <tbody>\n        `;\n        \n        let csvData = [];\n        \n        // Helper function to safely convert values to string\n        function safeStringify(value) {\n            if (value === null || value === undefined) return '';\n            if (typeof value === 'object') {\n                return JSON.stringify(value);\n            }\n            return String(value);\n        }\n        \n        // Flatten nested objects for better display\n        function flattenObject(obj, prefix = '') {\n            let flattened = {};\n            for (let key in obj) {\n                if (obj.hasOwnProperty(key)) {\n                    const fullKey = prefix ? `${prefix}.${key}` : key;\n                    const value = obj[key];\n                    \n                    if (typeof value === 'object' && value !== null && !Array.isArray(value)) {\n                        Object.assign(flattened, flattenObject(value, fullKey));\n                    } else {\n                        flattened[fullKey] = safeStringify(value);\n                    }\n                }\n            }\n            return flattened;\n        }\n        \n        factsArray.forEach((chunk, chunkIndex) => {\n            if (chunk.extracted_facts && !chunk.extracted_facts.error) {\n                const flattenedFacts = flattenObject(chunk.extracted_facts);\n                \n                Object.entries(flattenedFacts).forEach(([key, value]) => {\n                    if (key && value && value !== '') {\n                        html += `<tr><td><strong>${key}</strong></td><td>${value}</td><td>Text Chunk ${chunkIndex + 1}</td></tr>`;\n                        csvData.push({\n                            source: `Text Chunk ${chunkIndex + 1}`,\n                            type: 'Financial Data',\n                            field: key,\n                            value: value\n                        });\n                    }\n                });\n            }\n        });\n        \n        html += '</tbody></table></div>';\n        \n        return { html, csvData };\n    }\n    \n    function exportToCSV(data) {\n        if (!data || data.length === 0) {\n            alert('No data to export');\n            return;\n        }\n        \n        // Get all unique keys for CSV headers\n        const allKeys = new Set();\n        data.forEach(row => {\n            Object.keys(row).forEach(key => allKeys.add(key));\n        });\n        \n        const headers = Array.from(allKeys);\n        let csv = headers.join(',') + '\\n';\n        \n        data.forEach(row => {\n            const values = headers.map(header => {\n                const value = row[header] || '';\n                // Escape commas and quotes in CSV\n                return `\"${String(value).replace(/\"/g, '\"\"')}\"`;\n            });\n            csv += values.join(',') + '\\n';\n        });\n        \n        const blob = new Blob([csv], { type: 'text/csv' });\n        const url = URL.createObjectURL(blob);\n        const a = document.createElement('a');\n        a.href = url;\n        a.download = 'extracted_data.csv';\n        document.body.appendChild(a);\n        a.click();\n        document.body.removeChild(a);\n        URL.revokeObjectURL(url);\n    }\n    \n    function exportToExcel(data) {\n        // For now, export as CSV with .xlsx extension\n        // In a real implementation, you'd use a library like SheetJS\n        exportToCSV(data);\n        alert('Excel export completed as CSV format. For true Excel format, please use the CSV file with Excel.');\n    }\n\n    function showProcessingSummary(metadata) {\n        if (!metadata) return;\n        \n        const summaryHtml = `\n            <div class=\"processing-summary alert alert-success mb-3\">\n                <h5>Processing Complete</h5>\n                <div class=\"summary-stats\">\n                    <small class=\"text-muted\">\n                        Extracted and analyzed using ${metadata.processing_mode === 'agentic' ? 'advanced AI processing' : 'standard processing'}\n                        ${metadata.optimization && metadata.optimization.optimization_applied ? ' with table optimization applied' : ''}\n                    </small>\n                </div>\n            </div>\n        `;\n        \n        const resultsSection = document.querySelector('.results-section');\n        if (resultsSection) {\n            const existingSummary = resultsSection.querySelector('.processing-summary');\n            if (existingSummary) {\n                existingSummary.remove();\n            }\n            resultsSection.insertAdjacentHTML('afterbegin', summaryHtml);\n        }\n    }\n\n    function displayResults(data) {\n        displayResultsWithTables(data);\n    }\n\n    function displayResultsWithTables(data) {\n        if (!data.length) {\n            showError('No structured data could be extracted');\n            return;\n        }\n\n        // Clear previous results\n        resultsSection.innerHTML = `\n            <h4>Extracted Data with Commentary</h4>\n            <div id=\"tables-container\"></div>\n            <div id=\"data-table-container\">\n                <h5>All Data Points</h5>\n                <div class=\"table-responsive\">\n                    <table class=\"table table-striped table-hover\">\n                        <thead id=\"data-table-header\" class=\"table-dark\"></thead>\n                        <tbody id=\"data-table-body\"></tbody>\n                    </table>\n                </div>\n            </div>\n            <div class=\"mt-3\">\n                <button class=\"btn btn-outline-primary\" id=\"export-json-btn\">Export as JSON</button>\n                <button class=\"btn btn-outline-success\" id=\"export-csv-btn\">Export as CSV</button>\n                <button class=\"btn btn-outline-danger\" id=\"export-pdf-btn\">Export as PDF</button>\n            </div>\n        `;\n\n        const tablesContainer = document.getElementById('tables-container');\n        const dataTableHeader = document.getElementById('data-table-header');\n        const dataTableBody = document.getElementById('data-table-body');\n\n        // Reconstruct and display actual tables\n        const tableStructures = data.filter(row => row.is_table_header && row.headers && row.rows);\n        \n        tableStructures.forEach((tableHeader, index) => {\n            const tableDiv = document.createElement('div');\n            tableDiv.className = 'card mb-4';\n            \n            const headers = tableHeader.headers;\n            const rows = tableHeader.rows;\n            \n            // Determine table type for styling\n            const isDocumentTable = tableHeader.source && tableHeader.source.includes('Document Text');\n            const headerClass = isDocumentTable ? 'bg-success' : 'bg-primary';\n            const tableTitle = isDocumentTable ? \n                `Document Content Table ${index + 1}` : \n                `Extracted Table ${index + 1} (Page ${tableHeader.page})`;\n            \n            let tableHtml = `\n                <div class=\"card-header ${headerClass} text-white\">\n                    <h6 class=\"mb-0\">${tableTitle}</h6>\n                    <small>Columns: ${headers.length} | Rows: ${rows.length}</small>\n                </div>\n                <div class=\"card-body\">\n                    <div class=\"table-responsive\">\n                        <table class=\"table table-striped table-hover table-sm\">\n                            <thead class=\"table-light\">\n                                <tr>`;\n            \n            // Add headers\n            headers.forEach(header => {\n                tableHtml += `<th style=\"min-width: 120px;\">${header}</th>`;\n            });\n            \n            tableHtml += `</tr></thead><tbody>`;\n            \n            // Add rows\n            rows.forEach(row => {\n                tableHtml += '<tr>';\n                for (let i = 0; i < headers.length; i++) {\n                    const cellValue = i < row.length ? (row[i] || '-') : '-';\n                    tableHtml += `<td>${cellValue}</td>`;\n                }\n                tableHtml += '</tr>';\n            });\n            \n            tableHtml += `\n                            </tbody>\n                        </table>\n                    </div>\n                    <small class=\"text-muted\">\n                        ${isDocumentTable ? \n                            'Document content organized into structured table format' : \n                            'Original table reconstructed from extracted data points'}\n                    </small>\n                </div>\n            `;\n            \n            tableDiv.innerHTML = tableHtml;\n            tablesContainer.appendChild(tableDiv);\n        });\n\n        // Check if any row has commentary\n        const hasCommentary = data.some(row => row.commentary && row.commentary.trim());\n\n        // Create data table header with conditional commentary column\n        const headers = ['Source', 'Type', 'Field', 'Value', 'Page'];\n        if (hasCommentary) {\n            headers.push('Commentary');\n        }\n        \n        dataTableHeader.innerHTML = `\n            <tr>\n                ${headers.map(header => `<th>${header}</th>`).join('')}\n            </tr>\n        `;\n\n        // Create data table rows\n        data.forEach(row => {\n            // Skip table header rows in the detailed view\n            if (row.is_table_header) return;\n            \n            const tr = document.createElement('tr');\n            \n            // Add source column with badge\n            const sourceTd = document.createElement('td');\n            sourceTd.innerHTML = `<span class=\"badge bg-secondary\">${row.source || ''}</span>`;\n            tr.appendChild(sourceTd);\n            \n            // Add type column with badge\n            const typeTd = document.createElement('td');\n            const badgeClass = row.type === 'General Commentary' ? 'bg-warning' : \n                              row.type === 'Table Data' ? 'bg-success' : 'bg-info';\n            typeTd.innerHTML = `<span class=\"badge ${badgeClass}\">${row.type || ''}</span>`;\n            tr.appendChild(typeTd);\n            \n            // Add field column with bold text\n            const fieldTd = document.createElement('td');\n            fieldTd.innerHTML = `<strong>${row.field || ''}</strong>`;\n            tr.appendChild(fieldTd);\n            \n            // Add value column\n            const valueTd = document.createElement('td');\n            valueTd.textContent = row.value || '';\n            tr.appendChild(valueTd);\n            \n            // Add page column\n            const pageTd = document.createElement('td');\n            pageTd.textContent = row.page || '';\n            tr.appendChild(pageTd);\n            \n            // Add commentary column if needed\n            if (hasCommentary) {\n                const commentaryTd = document.createElement('td');\n                commentaryTd.className = 'commentary-cell';\n                if (row.commentary && row.commentary.trim()) {\n                    commentaryTd.innerHTML = `<span class=\"text-muted small\">${row.commentary}</span>`;\n                    tr.classList.add('has-commentary');\n                } else {\n                    commentaryTd.innerHTML = '<span class=\"text-muted\">-</span>';\n                }\n                tr.appendChild(commentaryTd);\n            }\n            \n            dataTableBody.appendChild(tr);\n        });\n\n        // Re-attach export event listeners\n        document.getElementById('export-json-btn').addEventListener('click', exportJson);\n        document.getElementById('export-csv-btn').addEventListener('click', exportCsv);\n        document.getElementById('export-pdf-btn').addEventListener('click', exportPdf);\n\n        // Store processed data globally\n        processedData = data;\n\n        // Show results section\n        resultsSection.classList.remove('d-none');\n        window.scrollTo({\n            top: resultsSection.offsetTop - 20,\n            behavior: 'smooth'\n        });\n    }\n\n    // Export functions\n    exportJsonBtn.addEventListener('click', exportJson);\n    exportCsvBtn.addEventListener('click', exportCsv);\n    exportPdfBtn.addEventListener('click', exportPdf);\n\n    function exportJson() {\n        if (!processedData.length) {\n            showError('No data to export');\n            return;\n        }\n\n        const jsonString = JSON.stringify(processedData, null, 2);\n        downloadFile(jsonString, 'extracted_data.json', 'application/json');\n    }\n\n    function exportCsv() {\n        if (!csvData) {\n            showError('No CSV data to export');\n            return;\n        }\n        \n        downloadFile(csvData, 'extracted_data_comments.csv', 'text/csv');\n    }\n    \n    function exportXlsx() {\n        if (!csvData) {\n            showError('No data to export');\n            return;\n        }\n\n        showLoading('Generating XLSX file...');\n\n        fetch('/export_xlsx', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({ csv_data: csvData })\n        })\n        .then(response => {\n            hideLoading();\n            \n            if (!response.ok) {\n                return response.json().then(data => {\n                    throw new Error(data.error || 'Failed to generate XLSX');\n                });\n            }\n            \n            // Handle binary file download\n            return response.blob();\n        })\n        .then(blob => {\n            // Create and click download link\n            const url = window.URL.createObjectURL(blob);\n            const link = document.createElement('a');\n            link.href = url;\n            link.download = 'extracted_data_comments.xlsx';\n            document.body.appendChild(link);\n            link.click();\n            document.body.removeChild(link);\n            window.URL.revokeObjectURL(url);\n        })\n        .catch(error => {\n            hideLoading();\n            showError(error.message);\n        });\n    }\n\n    function exportPdf() {\n        if (!processedData.length) {\n            showError('No data to export');\n            return;\n        }\n\n        showLoading('Generating PDF...');\n\n        fetch('/export/pdf', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({ data: processedData })\n        })\n        .then(response => {\n            if (!response.ok) {\n                return response.json().then(data => {\n                    throw new Error(data.error || 'Failed to generate PDF');\n                });\n            }\n            return response.json();\n        })\n        .then(data => {\n            hideLoading();\n            \n            if (data.pdf) {\n                // Create and click download link\n                const link = document.createElement('a');\n                link.href = `data:application/pdf;base64,${data.pdf}`;\n                link.download = 'extracted_data.pdf';\n                document.body.appendChild(link);\n                link.click();\n                document.body.removeChild(link);\n            } else {\n                throw new Error('No PDF data received from server');\n            }\n        })\n        .catch(error => {\n            hideLoading();\n            showError(error.message);\n        });\n    }\n\n    function downloadFile(content, fileName, contentType) {\n        const blob = new Blob([content], { type: contentType });\n        const url = URL.createObjectURL(blob);\n        \n        const link = document.createElement('a');\n        link.href = url;\n        link.download = fileName;\n        document.body.appendChild(link);\n        link.click();\n        document.body.removeChild(link);\n        \n        setTimeout(() => {\n            URL.revokeObjectURL(url);\n        }, 100);\n    }\n\n    // UI helpers\n    function showLoading(message) {\n        loadingMessage.textContent = message || 'Processing...';\n        loadingOverlay.classList.remove('d-none');\n        loadingOverlay.classList.add('active');\n    }\n\n    function hideLoading() {\n        loadingOverlay.classList.remove('active');\n        loadingOverlay.classList.add('d-none');\n    }\n\n    function showError(message) {\n        errorMessage.textContent = message;\n        errorMessage.classList.remove('d-none');\n        \n        setTimeout(() => {\n            errorMessage.classList.add('d-none');\n        }, 5000);\n    }\n\n    // Global function to show JSON modal\n    window.showJsonModal = function() {\n        if (!currentStructuredData) {\n            alert('No structured data available');\n            return;\n        }\n\n        // Create modal HTML\n        const modalHtml = `\n            <div class=\"modal fade\" id=\"jsonModal\" tabindex=\"-1\" aria-labelledby=\"jsonModalLabel\" aria-hidden=\"true\">\n                <div class=\"modal-dialog modal-xl\">\n                    <div class=\"modal-content\">\n                        <div class=\"modal-header\">\n                            <h5 class=\"modal-title\" id=\"jsonModalLabel\">Complete Amazon Textract JSON Structure</h5>\n                            <button type=\"button\" class=\"btn-close\" data-bs-dismiss=\"modal\" aria-label=\"Close\"></button>\n                        </div>\n                        <div class=\"modal-body\">\n                            <div class=\"mb-3\">\n                                <button class=\"btn btn-outline-secondary btn-sm\" onclick=\"copyJsonToClipboard()\">Copy JSON</button>\n                                <button class=\"btn btn-outline-primary btn-sm\" onclick=\"downloadJson()\">Download JSON</button>\n                            </div>\n                            <pre class=\"bg-light p-3 rounded\" style=\"max-height: 500px; overflow-y: auto; font-size: 12px;\"><code id=\"jsonContent\">${JSON.stringify(currentStructuredData, null, 2)}</code></pre>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        `;\n\n        // Remove existing modal if any\n        const existingModal = document.getElementById('jsonModal');\n        if (existingModal) {\n            existingModal.remove();\n        }\n\n        // Add modal to body\n        document.body.insertAdjacentHTML('beforeend', modalHtml);\n\n        // Show modal\n        const modal = new bootstrap.Modal(document.getElementById('jsonModal'));\n        modal.show();\n    };\n\n    window.copyJsonToClipboard = function() {\n        const jsonContent = JSON.stringify(currentStructuredData, null, 2);\n        navigator.clipboard.writeText(jsonContent).then(() => {\n            alert('JSON copied to clipboard!');\n        }).catch(() => {\n            alert('Failed to copy JSON to clipboard');\n        });\n    };\n\n    window.downloadJson = function() {\n        const jsonContent = JSON.stringify(currentStructuredData, null, 2);\n        const blob = new Blob([jsonContent], { type: 'application/json' });\n        const url = URL.createObjectURL(blob);\n        const a = document.createElement('a');\n        a.href = url;\n        a.download = 'textract_output.json';\n        document.body.appendChild(a);\n        a.click();\n        document.body.removeChild(a);\n        URL.revokeObjectURL(url);\n    };\n});","size_bytes":57931},"static/style.css":{"content":"/* Custom styles for PDF Text Extractor and Tabulator */\nbody {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    background-color: #f8f9fa;\n}\n\n.upload-area {\n    border: 2px dashed #dee2e6;\n    border-radius: 5px;\n    transition: all 0.3s ease;\n    cursor: pointer;\n}\n\n.upload-area:hover, .upload-area.dragover {\n    border-color: #0d6efd;\n    background-color: rgba(13, 110, 253, 0.05);\n}\n\n.extracted-text {\n    max-height: 300px;\n    overflow-y: auto;\n}\n\n.alert {\n    border-radius: 5px;\n}\n\n/* Table styles */\n.table {\n    width: 100%;\n    margin-bottom: 1rem;\n    background-color: transparent;\n}\n\n.table th {\n    font-weight: 600;\n}\n\n.table-responsive {\n    max-height: 400px;\n    overflow-y: auto;\n}\n\n/* Commentary column styles */\n.commentary-cell {\n    max-width: 250px;\n    word-wrap: break-word;\n    font-size: 0.9em;\n}\n\n.has-commentary {\n    background-color: rgba(25, 135, 84, 0.05);\n}\n\n.has-commentary:hover {\n    background-color: rgba(25, 135, 84, 0.1);\n}\n\n.commentary-cell .text-muted {\n    line-height: 1.4;\n}\n\n/* Loading spinner */\n#loading-overlay {\n    display: none;\n    position: fixed;\n    top: 0;\n    left: 0;\n    width: 100%;\n    height: 100%;\n    background-color: rgba(0, 0, 0, 0.7);\n    z-index: 9999;\n    justify-content: center;\n    align-items: center;\n    flex-direction: column;\n}\n\n#loading-overlay.active {\n    display: flex;\n}\n\n/* Responsive adjustments */\n@media (max-width: 768px) {\n    .export-options .d-flex {\n        flex-wrap: wrap;\n    }\n    .export-options button {\n        margin-bottom: 0.5rem;\n    }\n}","size_bytes":1568},"static/visualization.js":{"content":"class DataVisualization {\n    constructor() {\n        this.extractedText = '';\n        this.tabulatedData = [];\n        this.matchedSegments = new Set();\n        this.iterationHistory = [];\n        this.currentIteration = 0;\n    }\n\n    initialize(extractedText, tabulatedData, iterationHistory = []) {\n        this.extractedText = extractedText;\n        this.tabulatedData = tabulatedData;\n        this.iterationHistory = iterationHistory;\n        this.currentIteration = iterationHistory.length;\n        this.createVisualizationPanel();\n        this.updateVisualization();\n    }\n\n    createVisualizationPanel() {\n        // Remove existing panel if it exists\n        const existingPanel = document.getElementById('visualization-panel');\n        if (existingPanel) {\n            existingPanel.remove();\n        }\n\n        // Create main visualization panel\n        const panel = document.createElement('div');\n        panel.id = 'visualization-panel';\n        panel.className = 'visualization-panel';\n        panel.innerHTML = `\n            <div class=\"viz-header\">\n                <h3>🔍 Data Extraction Visualization</h3>\n                <div class=\"iteration-controls\">\n                    <span>Iteration: ${this.currentIteration}</span>\n                    <button id=\"prev-iteration\" class=\"iteration-btn\">‹ Previous</button>\n                    <button id=\"next-iteration\" class=\"iteration-btn\">Next ›</button>\n                </div>\n            </div>\n            \n            <div class=\"viz-content\">\n                <div class=\"viz-section\">\n                    <h4>📄 Original Text Analysis</h4>\n                    <div id=\"text-analysis\" class=\"text-container\"></div>\n                </div>\n                \n                <div class=\"viz-section\">\n                    <h4>📊 Coverage Statistics</h4>\n                    <div id=\"coverage-stats\" class=\"stats-container\"></div>\n                </div>\n                \n                <div class=\"viz-section\">\n                    <h4>🎯 Data Mapping</h4>\n                    <div id=\"data-mapping\" class=\"mapping-container\"></div>\n                </div>\n                \n                <div class=\"viz-section\">\n                    <h4>⚠️ Gaps & Improvements</h4>\n                    <div id=\"gaps-analysis\" class=\"gaps-container\"></div>\n                </div>\n            </div>\n        `;\n\n        // Add CSS styles\n        this.addVisualizationStyles();\n        \n        // Insert panel after the results section\n        const resultsSection = document.querySelector('.results-section');\n        if (resultsSection) {\n            resultsSection.parentNode.insertBefore(panel, resultsSection.nextSibling);\n        } else {\n            document.body.appendChild(panel);\n        }\n\n        // Add event listeners\n        this.addEventListeners();\n    }\n\n    addVisualizationStyles() {\n        if (document.getElementById('visualization-styles')) return;\n\n        const styles = document.createElement('style');\n        styles.id = 'visualization-styles';\n        styles.textContent = `\n            .visualization-panel {\n                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n                border-radius: 15px;\n                padding: 25px;\n                margin: 30px 0;\n                box-shadow: 0 15px 35px rgba(0,0,0,0.1);\n                color: white;\n            }\n            \n            .viz-header {\n                display: flex;\n                justify-content: space-between;\n                align-items: center;\n                margin-bottom: 25px;\n                padding-bottom: 15px;\n                border-bottom: 2px solid rgba(255,255,255,0.2);\n            }\n            \n            .viz-header h3 {\n                margin: 0;\n                font-size: 1.5em;\n                font-weight: bold;\n            }\n            \n            .iteration-controls {\n                display: flex;\n                align-items: center;\n                gap: 10px;\n            }\n            \n            .iteration-btn {\n                background: rgba(255,255,255,0.2);\n                border: 1px solid rgba(255,255,255,0.3);\n                color: white;\n                padding: 8px 15px;\n                border-radius: 8px;\n                cursor: pointer;\n                font-size: 14px;\n                transition: all 0.3s ease;\n            }\n            \n            .iteration-btn:hover {\n                background: rgba(255,255,255,0.3);\n                transform: translateY(-2px);\n            }\n            \n            .iteration-btn:disabled {\n                opacity: 0.5;\n                cursor: not-allowed;\n                transform: none;\n            }\n            \n            .viz-content {\n                display: grid;\n                grid-template-columns: 1fr 1fr;\n                gap: 20px;\n            }\n            \n            .viz-section {\n                background: rgba(255,255,255,0.1);\n                border-radius: 10px;\n                padding: 20px;\n                backdrop-filter: blur(10px);\n            }\n            \n            .viz-section h4 {\n                margin: 0 0 15px 0;\n                font-size: 1.1em;\n                color: #fff;\n            }\n            \n            .text-container {\n                max-height: 300px;\n                overflow-y: auto;\n                background: rgba(0,0,0,0.2);\n                padding: 15px;\n                border-radius: 8px;\n                font-family: 'Courier New', monospace;\n                font-size: 12px;\n                line-height: 1.4;\n            }\n            \n            .highlighted-text {\n                background: rgba(46, 204, 113, 0.3);\n                border-left: 3px solid #2ecc71;\n                padding: 2px 4px;\n                margin: 1px 0;\n                border-radius: 3px;\n            }\n            \n            .unmatched-text {\n                background: rgba(231, 76, 60, 0.3);\n                border-left: 3px solid #e74c3c;\n                padding: 2px 4px;\n                margin: 1px 0;\n                border-radius: 3px;\n            }\n            \n            .stats-container {\n                display: grid;\n                grid-template-columns: 1fr 1fr;\n                gap: 15px;\n            }\n            \n            .stat-item {\n                background: rgba(0,0,0,0.2);\n                padding: 15px;\n                border-radius: 8px;\n                text-align: center;\n            }\n            \n            .stat-value {\n                font-size: 2em;\n                font-weight: bold;\n                color: #2ecc71;\n            }\n            \n            .stat-label {\n                font-size: 0.9em;\n                opacity: 0.8;\n                margin-top: 5px;\n            }\n            \n            .coverage-bar {\n                width: 100%;\n                height: 10px;\n                background: rgba(0,0,0,0.3);\n                border-radius: 5px;\n                overflow: hidden;\n                margin: 10px 0;\n            }\n            \n            .coverage-fill {\n                height: 100%;\n                background: linear-gradient(90deg, #e74c3c, #f39c12, #2ecc71);\n                transition: width 0.5s ease;\n                border-radius: 5px;\n            }\n            \n            .mapping-container {\n                max-height: 300px;\n                overflow-y: auto;\n            }\n            \n            .mapping-item {\n                background: rgba(0,0,0,0.2);\n                padding: 10px;\n                margin: 8px 0;\n                border-radius: 6px;\n                border-left: 4px solid #3498db;\n            }\n            \n            .mapping-source {\n                font-size: 0.85em;\n                opacity: 0.8;\n                margin-bottom: 5px;\n            }\n            \n            .mapping-target {\n                font-weight: bold;\n            }\n            \n            .gaps-container {\n                max-height: 300px;\n                overflow-y: auto;\n            }\n            \n            .gap-item {\n                background: rgba(231, 76, 60, 0.2);\n                border: 1px solid rgba(231, 76, 60, 0.4);\n                padding: 12px;\n                margin: 8px 0;\n                border-radius: 6px;\n            }\n            \n            .gap-type {\n                font-weight: bold;\n                color: #e74c3c;\n                font-size: 0.9em;\n            }\n            \n            .gap-description {\n                margin-top: 5px;\n                font-size: 0.85em;\n            }\n            \n            .iteration-summary {\n                background: rgba(52, 152, 219, 0.2);\n                border: 1px solid rgba(52, 152, 219, 0.4);\n                padding: 15px;\n                border-radius: 8px;\n                margin-bottom: 20px;\n            }\n            \n            .pulse-animation {\n                animation: pulse 2s infinite;\n            }\n            \n            @keyframes pulse {\n                0% { opacity: 1; }\n                50% { opacity: 0.7; }\n                100% { opacity: 1; }\n            }\n        `;\n        \n        document.head.appendChild(styles);\n    }\n\n    addEventListeners() {\n        const prevBtn = document.getElementById('prev-iteration');\n        const nextBtn = document.getElementById('next-iteration');\n        \n        if (prevBtn) {\n            prevBtn.addEventListener('click', () => this.showPreviousIteration());\n        }\n        \n        if (nextBtn) {\n            nextBtn.addEventListener('click', () => this.showNextIteration());\n        }\n        \n        this.updateIterationButtons();\n    }\n\n    updateIterationButtons() {\n        const prevBtn = document.getElementById('prev-iteration');\n        const nextBtn = document.getElementById('next-iteration');\n        \n        if (prevBtn) prevBtn.disabled = this.currentIteration <= 1;\n        if (nextBtn) nextBtn.disabled = this.currentIteration >= this.iterationHistory.length;\n    }\n\n    showPreviousIteration() {\n        if (this.currentIteration > 1) {\n            this.currentIteration--;\n            this.updateVisualization();\n            this.updateIterationButtons();\n        }\n    }\n\n    showNextIteration() {\n        if (this.currentIteration < this.iterationHistory.length) {\n            this.currentIteration++;\n            this.updateVisualization();\n            this.updateIterationButtons();\n        }\n    }\n\n    updateVisualization() {\n        this.updateTextAnalysis();\n        this.updateCoverageStats();\n        this.updateDataMapping();\n        this.updateGapsAnalysis();\n    }\n\n    updateTextAnalysis() {\n        const container = document.getElementById('text-analysis');\n        if (!container) return;\n\n        // Analyze text segments and highlight them\n        const segments = this.extractTextSegments();\n        const highlightedText = this.highlightTextSegments(segments);\n        \n        container.innerHTML = highlightedText;\n    }\n\n    extractTextSegments() {\n        // Split text into meaningful segments (sentences, phrases, data points)\n        const sentences = this.extractedText.split(/[.!?]+/).filter(s => s.trim().length > 0);\n        const dataPoints = this.extractedText.match(/\\$[\\d,]+\\.?\\d*|\\d+%|\\d{4}-\\d{2}-\\d{2}|\\b\\d+\\.\\d+\\b/g) || [];\n        \n        return {\n            sentences: sentences.map(s => s.trim()),\n            dataPoints: dataPoints\n        };\n    }\n\n    highlightTextSegments(segments) {\n        let highlightedText = this.extractedText;\n        \n        // Highlight matched data points\n        segments.dataPoints.forEach(point => {\n            const isMatched = this.isDataPointMatched(point);\n            const className = isMatched ? 'highlighted-text' : 'unmatched-text';\n            highlightedText = highlightedText.replace(\n                new RegExp(this.escapeRegex(point), 'g'),\n                `<span class=\"${className}\">${point}</span>`\n            );\n        });\n\n        return highlightedText;\n    }\n\n    isDataPointMatched(dataPoint) {\n        // Check if data point appears in tabulated data\n        const tableText = JSON.stringify(this.tabulatedData).toLowerCase();\n        return tableText.includes(dataPoint.toLowerCase().replace(/[$,]/g, ''));\n    }\n\n    updateCoverageStats() {\n        const container = document.getElementById('coverage-stats');\n        if (!container) return;\n\n        const currentIteration = this.getCurrentIterationData();\n        const coverage = currentIteration?.tabulation?.coverage_analysis || {\n            total_data_points: 0,\n            categorized_points: 0,\n            coverage_percentage: 0\n        };\n\n        container.innerHTML = `\n            <div class=\"stat-item\">\n                <div class=\"stat-value\">${coverage.total_data_points}</div>\n                <div class=\"stat-label\">Data Points Found</div>\n            </div>\n            <div class=\"stat-item\">\n                <div class=\"stat-value\">${coverage.categorized_points}</div>\n                <div class=\"stat-label\">Points Categorized</div>\n            </div>\n            <div class=\"coverage-bar\">\n                <div class=\"coverage-fill\" style=\"width: ${coverage.coverage_percentage}%\"></div>\n            </div>\n            <div style=\"text-align: center; margin-top: 10px;\">\n                <strong>${coverage.coverage_percentage}% Coverage</strong>\n            </div>\n        `;\n    }\n\n    updateDataMapping() {\n        const container = document.getElementById('data-mapping');\n        if (!container) return;\n\n        const mappings = this.generateDataMappings();\n        \n        container.innerHTML = mappings.map(mapping => `\n            <div class=\"mapping-item\">\n                <div class=\"mapping-source\">Source: \"${mapping.source}\"</div>\n                <div class=\"mapping-target\">→ ${mapping.category}: ${mapping.value}</div>\n            </div>\n        `).join('');\n    }\n\n    generateDataMappings() {\n        const mappings = [];\n        \n        this.tabulatedData.forEach(row => {\n            const category = row.Category || 'Unknown';\n            const values = Object.keys(row)\n                .filter(key => key.startsWith('Value'))\n                .map(key => row[key])\n                .filter(value => value && value.trim());\n            \n            values.forEach(value => {\n                // Try to find this value in the original text\n                const sourceMatch = this.findSourceInText(value);\n                mappings.push({\n                    source: sourceMatch || 'Auto-detected',\n                    category: category,\n                    value: value\n                });\n            });\n        });\n        \n        return mappings.slice(0, 10); // Show first 10 mappings\n    }\n\n    findSourceInText(value) {\n        // Find the context around a value in the original text\n        const valueRegex = new RegExp(this.escapeRegex(value), 'i');\n        const match = this.extractedText.match(valueRegex);\n        \n        if (match) {\n            const index = this.extractedText.indexOf(match[0]);\n            const start = Math.max(0, index - 50);\n            const end = Math.min(this.extractedText.length, index + match[0].length + 50);\n            return this.extractedText.substring(start, end).trim();\n        }\n        \n        return null;\n    }\n\n    updateGapsAnalysis() {\n        const container = document.getElementById('gaps-analysis');\n        if (!container) return;\n\n        const currentIteration = this.getCurrentIterationData();\n        const gaps = this.analyzeGaps(currentIteration);\n\n        container.innerHTML = gaps.map(gap => `\n            <div class=\"gap-item\">\n                <div class=\"gap-type\">${gap.type}</div>\n                <div class=\"gap-description\">${gap.description}</div>\n            </div>\n        `).join('');\n    }\n\n    analyzeGaps(iterationData) {\n        const gaps = [];\n        \n        if (iterationData?.verification) {\n            try {\n                const verification = typeof iterationData.verification === 'string' \n                    ? JSON.parse(iterationData.verification) \n                    : iterationData.verification;\n                \n                if (verification.missing_information) {\n                    verification.missing_information.forEach(item => {\n                        gaps.push({\n                            type: 'Missing Data',\n                            description: item\n                        });\n                    });\n                }\n                \n                if (verification.mismatches) {\n                    verification.mismatches.forEach(item => {\n                        gaps.push({\n                            type: 'Data Mismatch',\n                            description: item\n                        });\n                    });\n                }\n            } catch (e) {\n                gaps.push({\n                    type: 'Analysis',\n                    description: 'Verification data processing needed'\n                });\n            }\n        }\n        \n        if (gaps.length === 0) {\n            gaps.push({\n                type: 'Status',\n                description: 'No significant gaps identified in current iteration'\n            });\n        }\n        \n        return gaps;\n    }\n\n    getCurrentIterationData() {\n        if (this.iterationHistory.length === 0) return null;\n        return this.iterationHistory[this.currentIteration - 1] || this.iterationHistory[this.iterationHistory.length - 1];\n    }\n\n    escapeRegex(string) {\n        return string.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\n    }\n\n    showIterationProgress(iteration, total) {\n        // Add visual feedback for iteration progress\n        const header = document.querySelector('.viz-header h3');\n        if (header) {\n            header.innerHTML = `🔍 Data Extraction Visualization <span class=\"pulse-animation\">(Processing ${iteration}/${total})</span>`;\n        }\n    }\n\n    hideIterationProgress() {\n        const header = document.querySelector('.viz-header h3');\n        if (header) {\n            header.innerHTML = '🔍 Data Extraction Visualization';\n        }\n    }\n}\n\n// Global instance\nwindow.dataVisualization = new DataVisualization();","size_bytes":18200}},"version":1}